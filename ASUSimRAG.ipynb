{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.data import Data\n",
    "from pypdf import PdfReader\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from openai import OpenAI\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import faiss\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from itertools import chain\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set to DEBUG for detailed logs\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('rag_system.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"ASUSimRAG\")\n",
    "\n",
    "# Define device for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class PDFTextExtractor:\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "\n",
    "    def extract_text(self) -> str:\n",
    "        \"\"\"Extract and clean text from a PDF file.\"\"\"\n",
    "        reader = PdfReader(self.pdf_path)\n",
    "        text = \"\"\n",
    "        for page_num, page in enumerate(reader.pages, start=1):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "            logger.debug(f\"Extracted text from page {page_num}.\")\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "        logger.debug(\"Extracted and cleaned text from PDF.\")\n",
    "        return text\n",
    "\n",
    "\n",
    "class SentenceProcessor:\n",
    "    def __init__(self, text: str, nlp: Language):\n",
    "        self.text = text\n",
    "        self.nlp = nlp\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.sentences = []\n",
    "        self.processed_docs = []\n",
    "\n",
    "    def split_into_sentences(self) -> List[str]:\n",
    "        \"\"\"Split text into sentences using spaCy with custom boundary detection.\"\"\"\n",
    "        # Create a new DocBin to store the modified document\n",
    "        doc = self.nlp(self.text)\n",
    "        \n",
    "        # Instead of modifying tokens directly, collect sentence boundaries\n",
    "        sentence_boundaries = set()\n",
    "        for sent in doc.sents:\n",
    "            sentence_boundaries.add(sent.start)\n",
    "        \n",
    "        # Add semicolons as additional sentence boundaries\n",
    "        for token in doc:\n",
    "            if token.text == ';':\n",
    "                next_token_idx = token.i + 1\n",
    "                if next_token_idx < len(doc):\n",
    "                    sentence_boundaries.add(next_token_idx)\n",
    "        \n",
    "        # Create sentences based on collected boundaries\n",
    "        sentence_starts = sorted(list(sentence_boundaries))\n",
    "        sentence_starts.append(len(doc))  # Add document end as final boundary\n",
    "        \n",
    "        # Extract sentences using the boundaries\n",
    "        sentences = []\n",
    "        for i in range(len(sentence_starts) - 1):\n",
    "            start = sentence_starts[i]\n",
    "            end = sentence_starts[i + 1]\n",
    "            sent_text = doc[start:end].text.strip()\n",
    "            if sent_text:  # Only add non-empty sentences\n",
    "                sentences.append(sent_text)\n",
    "        \n",
    "        self.sentences = sentences\n",
    "        logger.debug(f\"Split text into {len(self.sentences)} sentences.\")\n",
    "        return self.sentences\n",
    "\n",
    "    def preprocess_sentences(self) -> List[Doc]:\n",
    "        \"\"\"Preprocess sentences with lemmatization, POS tagging, and named entity recognition.\"\"\"\n",
    "        def preprocess(sent: str) -> Doc:\n",
    "            doc = self.nlp(sent)\n",
    "            return doc\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            self.processed_docs = list(executor.map(preprocess, self.sentences))\n",
    "        logger.debug(\"Preprocessed sentences with spaCy.\")\n",
    "        return self.processed_docs\n",
    "\n",
    "\n",
    "class EnhancedKnowledgeGraphBuilder:\n",
    "    def __init__(self, processed_docs: List[Doc], sentences: List[str], ontology_path: str = None):\n",
    "        self.processed_docs = processed_docs\n",
    "        self.sentences = sentences\n",
    "        self.ontology_path = ontology_path\n",
    "        self.knowledge_graph = nx.MultiDiGraph()\n",
    "        self.entity_relations = defaultdict(list)\n",
    "        self.ontology = self.load_ontology()\n",
    "        self.build_knowledge_graph()\n",
    "\n",
    "    def load_ontology(self):\n",
    "        \"\"\"Load an ontology to enrich the knowledge graph.\"\"\"\n",
    "        if self.ontology_path and os.path.exists(self.ontology_path):\n",
    "            with open(self.ontology_path, 'rb') as f:\n",
    "                ontology = pickle.load(f)\n",
    "            logger.debug(\"Loaded ontology for knowledge graph enrichment.\")\n",
    "            return ontology\n",
    "        logger.debug(\"No ontology path provided or file does not exist.\")\n",
    "        return None\n",
    "\n",
    "    def extract_relations(self, doc: Doc) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"Extract relations using dependency parsing and pattern matching.\"\"\"\n",
    "        relations = []\n",
    "        for sent in doc.sents:\n",
    "            subj = \"\"\n",
    "            verb = \"\"\n",
    "            obj = \"\"\n",
    "            for token in sent:\n",
    "                if token.dep_ in ('nsubj', 'nsubjpass'):\n",
    "                    subj = token.text\n",
    "                    verb = token.head.lemma_\n",
    "                    for child in token.head.children:\n",
    "                        if child.dep_ == 'dobj':\n",
    "                            obj = child.text\n",
    "                            relations.append((subj, verb, obj))\n",
    "                            logger.debug(f\"Extracted relation: {subj} -[{verb}]-> {obj}\")\n",
    "        return relations\n",
    "\n",
    "    def enrich_with_ontology(self, entity: str, label: str):\n",
    "        \"\"\"Enrich nodes with ontology-based relationships.\"\"\"\n",
    "        if self.ontology and entity in self.ontology:\n",
    "            for related_entity, relation in self.ontology[entity]:\n",
    "                self.add_node_if_not_exists(related_entity, label='Ontology')\n",
    "                self.knowledge_graph.add_edge(entity, related_entity, key=relation, label=relation)\n",
    "                logger.debug(f\"Enriched graph with ontology relation: {entity} -[{relation}]-> {related_entity}\")\n",
    "\n",
    "    def add_node_if_not_exists(self, node_text: str, label: str = None, sentence: str = None):\n",
    "        \"\"\"Add a node to the knowledge graph if it doesn't exist, ensuring 'sentences' attribute is initialized.\"\"\"\n",
    "        if not self.knowledge_graph.has_node(node_text):\n",
    "            self.knowledge_graph.add_node(node_text, sentences=[], label=label)\n",
    "            logger.debug(f\"Added node: {node_text} with label: {label}\")\n",
    "        if sentence:\n",
    "            self.knowledge_graph.nodes[node_text]['sentences'].append(sentence)\n",
    "        if label and 'label' not in self.knowledge_graph.nodes[node_text]:\n",
    "            self.knowledge_graph.nodes[node_text]['label'] = label\n",
    "\n",
    "    def build_knowledge_graph(self):\n",
    "        \"\"\"Construct a knowledge graph using advanced NLP techniques and ontology enrichment.\"\"\"\n",
    "        for idx, doc in enumerate(self.processed_docs):\n",
    "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "            for ent_text, ent_label in entities:\n",
    "                self.add_node_if_not_exists(ent_text, label=ent_label, sentence=self.sentences[idx])\n",
    "                self.enrich_with_ontology(ent_text, ent_label)\n",
    "\n",
    "            # Extract relations\n",
    "            relations = self.extract_relations(doc)\n",
    "            for subj, rel, obj in relations:\n",
    "                self.add_node_if_not_exists(subj)\n",
    "                self.add_node_if_not_exists(obj)\n",
    "                self.knowledge_graph.add_edge(subj, obj, key=rel, label='relation')\n",
    "                self.entity_relations[subj].append((obj, rel))\n",
    "                logger.debug(f\"Added edge: {subj} -[{rel}]-> {obj}\")\n",
    "\n",
    "        logger.info(f\"Constructed knowledge graph with {self.knowledge_graph.number_of_nodes()} nodes and {self.knowledge_graph.number_of_edges()} edges.\")\n",
    "\n",
    "    def get_knowledge_graph(self) -> nx.MultiDiGraph:\n",
    "        \"\"\"Return the constructed knowledge graph.\"\"\"\n",
    "        return self.knowledge_graph\n",
    "\n",
    "    def convert_to_pyg_data(self):\n",
    "        \"\"\"Convert the NetworkX graph to PyTorch Geometric data format for GNN processing.\"\"\"\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(self.knowledge_graph.nodes())}\n",
    "        idx_to_node = {idx: node for node, idx in node_to_idx.items()}\n",
    "        edge_index = []\n",
    "        edge_type = []\n",
    "\n",
    "        for u, v, data in self.knowledge_graph.edges(data=True):\n",
    "            u_idx = node_to_idx[u]\n",
    "            v_idx = node_to_idx[v]\n",
    "            edge_index.append([u_idx, v_idx])\n",
    "            # Encode edge types using a unique integer identifier\n",
    "            relation = data.get('key', 'relation')\n",
    "            edge_type.append(self.get_relation_id(relation))\n",
    "\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "\n",
    "        # Initialize node features using pre-trained embeddings (e.g., SentenceTransformer)\n",
    "        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        node_embeddings = embedding_model.encode(list(node_to_idx.keys()), convert_to_tensor=True)\n",
    "        node_embeddings = node_embeddings.to(device)\n",
    "\n",
    "        data = Data(x=node_embeddings, edge_index=edge_index, edge_type=edge_type)\n",
    "        logger.debug(\"Converted NetworkX graph to PyTorch Geometric Data object.\")\n",
    "        return data, idx_to_node, node_to_idx\n",
    "\n",
    "    def get_relation_id(self, relation: str) -> int:\n",
    "        \"\"\"Assign a unique ID to each relation type.\"\"\"\n",
    "        if not hasattr(self, 'relation_to_id'):\n",
    "            self.relation_to_id = {}\n",
    "            self.next_relation_id = 0\n",
    "        if relation not in self.relation_to_id:\n",
    "            self.relation_to_id[relation] = self.next_relation_id\n",
    "            self.next_relation_id += 1\n",
    "        return self.relation_to_id[relation]\n",
    "\n",
    "\n",
    "class AdvancedGNNReasoner(torch.nn.Module):\n",
    "    def __init__(self, num_node_features: int, num_relations: int, hidden_channels: int = 512, out_channels: int = 256):\n",
    "        super(AdvancedGNNReasoner, self).__init__()\n",
    "        # Relational GCN to handle different edge types\n",
    "        self.conv1 = RGCNConv(num_node_features, hidden_channels, num_relations)\n",
    "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations)\n",
    "        self.conv3 = RGCNConv(hidden_channels, out_channels, num_relations)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "\n",
    "        # First RGCN layer with activation and dropout\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second RGCN layer\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third RGCN layer\n",
    "        x = self.conv3(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, sentences: List[str], embeddings_path: str = None):\n",
    "        self.sentences = sentences\n",
    "        self.text_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.sentence_embeddings = None\n",
    "        self.index = None\n",
    "        self.embeddings_path = embeddings_path\n",
    "        self.compute_embeddings()\n",
    "        self.build_faiss_index()\n",
    "\n",
    "    def compute_embeddings(self):\n",
    "        \"\"\"Compute or load embeddings for all sentences.\"\"\"\n",
    "        if self.embeddings_path and os.path.exists(self.embeddings_path):\n",
    "            try:\n",
    "                with open(self.embeddings_path, 'rb') as f:\n",
    "                    self.sentence_embeddings = pickle.load(f)\n",
    "                logger.info(\"Loaded sentence embeddings from disk.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading embeddings: {e}\")\n",
    "                self.sentence_embeddings = None\n",
    "        \n",
    "        if self.sentence_embeddings is None:\n",
    "            # Compute embeddings in batches\n",
    "            batch_size = 32\n",
    "            all_embeddings = []\n",
    "            for i in range(0, len(self.sentences), batch_size):\n",
    "                batch = self.sentences[i:i + batch_size]\n",
    "                batch_embeddings = self.text_model.encode(batch, convert_to_tensor=True)\n",
    "                all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            \n",
    "            self.sentence_embeddings = np.vstack(all_embeddings)\n",
    "            \n",
    "            if self.embeddings_path:\n",
    "                try:\n",
    "                    with open(self.embeddings_path, 'wb') as f:\n",
    "                        pickle.dump(self.sentence_embeddings, f)\n",
    "                    logger.info(\"Saved sentence embeddings to disk.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error saving embeddings: {e}\")\n",
    "\n",
    "        return self.sentence_embeddings\n",
    "\n",
    "    def build_faiss_index(self):\n",
    "        \"\"\"Build a FAISS index for efficient similarity search.\"\"\"\n",
    "        try:\n",
    "            dimension = self.sentence_embeddings.shape[1]\n",
    "            \n",
    "            # Use IndexFlatL2 for more stable results\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "            \n",
    "            # Normalize the embeddings\n",
    "            embeddings = self.sentence_embeddings.copy()\n",
    "            faiss.normalize_L2(embeddings)\n",
    "            \n",
    "            # Add to index\n",
    "            self.index.add(embeddings)\n",
    "            \n",
    "            logger.debug(f\"Built FAISS index with {self.index.ntotal} vectors of dimension {dimension}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building FAISS index: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class SemanticSearcher:\n",
    "    def __init__(self, embedding_manager: EmbeddingManager):\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.index = self.embedding_manager.index\n",
    "        self.logger = logging.getLogger(\"ASUSimRAG\")\n",
    "\n",
    "    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Perform semantic search using FAISS index.\"\"\"\n",
    "        try:\n",
    "            self.logger.debug(f\"Starting semantic search for query: {query}\")\n",
    "            \n",
    "            # Encode query\n",
    "            query_embedding = self.embedding_manager.text_model.encode(\n",
    "                [query], \n",
    "                convert_to_tensor=True,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            query_embedding = query_embedding.cpu().numpy()\n",
    "            \n",
    "            # Perform search\n",
    "            actual_k = min(top_k, self.index.ntotal)\n",
    "            distances, indices = self.index.search(query_embedding, actual_k)\n",
    "            \n",
    "            # Convert distances to similarity scores (since we're using L2 distance)\n",
    "            # Lower L2 distance means higher similarity\n",
    "            max_dist = np.max(distances) + 1e-6  # Avoid division by zero\n",
    "            similarities = 1 - distances / max_dist\n",
    "            \n",
    "            # Create results\n",
    "            results = []\n",
    "            for idx, sim in zip(indices[0], similarities[0]):\n",
    "                if 0 <= idx < len(self.embedding_manager.sentences):\n",
    "                    results.append((int(idx), float(sim)))\n",
    "                    self.logger.debug(f\"Match {idx}: {self.embedding_manager.sentences[idx][:100]}... (score: {sim:.3f})\")\n",
    "            \n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in semantic search: {str(e)}\")\n",
    "            self.logger.debug(traceback.format_exc())\n",
    "            return [(0, 1.0)]  # Return first sentence as fallback\n",
    "\n",
    "\n",
    "class ContextBuilder:\n",
    "    def __init__(\n",
    "        self, \n",
    "        sentences: List[str], \n",
    "        knowledge_graph: nx.MultiDiGraph, \n",
    "        encoder: SentenceTransformer, \n",
    "        processed_docs: List[Doc],\n",
    "        gnn_data: Data,\n",
    "        idx_to_node: Dict[int, str],\n",
    "        node_to_idx: Dict[str, int]\n",
    "    ):\n",
    "        self.sentences = sentences\n",
    "        self.knowledge_graph = knowledge_graph\n",
    "        self.encoder = encoder\n",
    "        self.processed_docs = processed_docs\n",
    "        self.gnn_data = gnn_data\n",
    "        self.idx_to_node = idx_to_node\n",
    "        self.node_to_idx = node_to_idx\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.sentence_indices = {sent: idx for idx, sent in enumerate(self.sentences)}\n",
    "        \n",
    "        try:\n",
    "            self.sentence_graph = self.build_sentence_graph()\n",
    "            logger.debug(\"Built sentence graph successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building sentence graph: {str(e)}\")\n",
    "            self.sentence_graph = nx.Graph()\n",
    "            \n",
    "        # Initialize projection layer\n",
    "        try:\n",
    "            self.projection = torch.nn.Linear(768, 256).to(device)\n",
    "            logger.debug(\"Initialized projection layer\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing projection layer: {str(e)}\")\n",
    "            # Create a simple fallback projection\n",
    "            self.projection = lambda x: x[:, :256] if x.shape[1] > 256 else x\n",
    "            \n",
    "        try:\n",
    "            # Load or compute entity embeddings with error handling\n",
    "            self.entity_nodes, self.entity_embeddings = self.load_or_compute_entity_embeddings()\n",
    "            logger.debug(f\"Loaded entity embeddings: {self.entity_embeddings.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in entity embeddings initialization: {str(e)}\")\n",
    "            # Initialize with empty embeddings that won't break downstream\n",
    "            self.entity_nodes = []\n",
    "            self.entity_embeddings = np.zeros((0, 256))\n",
    "\n",
    "    def build_sentence_graph(self) -> nx.Graph:\n",
    "        \"\"\"Build a graph where nodes are sentences and edges represent similarity.\"\"\"\n",
    "        try:\n",
    "            graph_path = 'sentence_graph.pkl'\n",
    "            if os.path.exists(graph_path):\n",
    "                with open(graph_path, 'rb') as f:\n",
    "                    sentence_graph = pickle.load(f)\n",
    "                logger.info(\"Loaded sentence graph from disk.\")\n",
    "            else:\n",
    "                sentence_graph = nx.Graph()\n",
    "                \n",
    "                # First add all nodes to ensure complete index coverage\n",
    "                for idx in range(len(self.sentences)):\n",
    "                    sentence_graph.add_node(idx)\n",
    "                \n",
    "                # Compute embeddings for similarity calculation\n",
    "                embeddings = self.get_sentence_embeddings()\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "                \n",
    "                # Calculate similarities and add edges\n",
    "                similarity_matrix = cosine_similarity(embeddings)\n",
    "                threshold = 0.60  # Similarity threshold for edge creation\n",
    "                \n",
    "                # Add edges based on similarity\n",
    "                for i in range(len(self.sentences)):\n",
    "                    for j in range(i + 1, len(self.sentences)):\n",
    "                        if similarity_matrix[i][j] > threshold:\n",
    "                            sentence_graph.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "                        \n",
    "                        # Always add edges to adjacent sentences with lower weight\n",
    "                        # This ensures connectivity for context windows\n",
    "                        if j == i + 1:\n",
    "                            if not sentence_graph.has_edge(i, j):\n",
    "                                sentence_graph.add_edge(i, j, weight=0.5)\n",
    "                \n",
    "                # Verify graph properties\n",
    "                if not nx.is_connected(sentence_graph):\n",
    "                    logger.warning(\"Sentence graph is not fully connected. Adding minimal connecting edges.\")\n",
    "                    components = list(nx.connected_components(sentence_graph))\n",
    "                    for i in range(len(components) - 1):\n",
    "                        # Connect components with minimal edges\n",
    "                        comp1 = list(components[i])[0]\n",
    "                        comp2 = list(components[i + 1])[0]\n",
    "                        sentence_graph.add_edge(comp1, comp2, weight=0.1)\n",
    "                \n",
    "                # Save the graph\n",
    "                with open(graph_path, 'wb') as f:\n",
    "                    pickle.dump(sentence_graph, f)\n",
    "                logger.info(\"Saved sentence graph to disk.\")\n",
    "                \n",
    "            logger.debug(\"Built sentence graph based on similarity.\")\n",
    "            return sentence_graph\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building sentence graph: {str(e)}\")\n",
    "            logger.debug(traceback.format_exc())\n",
    "            \n",
    "            # Return minimal fallback graph if error occurs\n",
    "            fallback_graph = nx.Graph()\n",
    "            for i in range(len(self.sentences)):\n",
    "                fallback_graph.add_node(i)\n",
    "                if i > 0:\n",
    "                    fallback_graph.add_edge(i-1, i, weight=0.5)\n",
    "            return fallback_graph\n",
    "\n",
    "    def get_sentence_embeddings(self, sentences=None):\n",
    "        \"\"\"Compute or load sentence embeddings for graph construction.\"\"\"\n",
    "        if sentences is None:\n",
    "            embeddings_path = 'sentence_embeddings_for_graph.pkl'\n",
    "            if os.path.exists(embeddings_path):\n",
    "                with open(embeddings_path, 'rb') as f:\n",
    "                    embeddings = pickle.load(f)\n",
    "                logger.info(\"Loaded sentence embeddings for graph from disk.\")\n",
    "            else:\n",
    "                embeddings = self.encoder.encode(self.sentences, convert_to_tensor=True)\n",
    "                if embeddings.device.type != 'cpu':\n",
    "                    embeddings = embeddings.cpu()\n",
    "                with open(embeddings_path, 'wb') as f:\n",
    "                    pickle.dump(embeddings.cpu().numpy(), f)\n",
    "                logger.info(\"Saved sentence embeddings for graph to disk.\")\n",
    "        else:\n",
    "            # For new sentences, compute embeddings\n",
    "            embeddings = self.encoder.encode(sentences, convert_to_tensor=True)\n",
    "            if embeddings.device.type != 'cpu':\n",
    "                embeddings = embeddings.cpu()\n",
    "        return embeddings\n",
    "\n",
    "    def load_or_compute_entity_embeddings(self):\n",
    "        \"\"\"Compute or load embeddings for entity sentences with robust error handling.\"\"\"\n",
    "        try:\n",
    "            entity_embeddings_path = 'entity_embeddings.pkl'\n",
    "            if os.path.exists(entity_embeddings_path):\n",
    "                with open(entity_embeddings_path, 'rb') as f:\n",
    "                    entity_nodes, entity_embeddings = pickle.load(f)\n",
    "                logger.info(\"Loaded entity embeddings from disk.\")\n",
    "            else:\n",
    "                entity_nodes = list(self.knowledge_graph.nodes)\n",
    "                # More robust sentence extraction with fallback\n",
    "                entity_sentences = []\n",
    "                for node in entity_nodes:\n",
    "                    node_data = self.knowledge_graph.nodes[node]\n",
    "                    sentences = node_data.get('sentences', [])\n",
    "                    # If no sentences, use the node text itself\n",
    "                    if not sentences:\n",
    "                        entity_sentences.append(str(node))\n",
    "                    else:\n",
    "                        # Take the first non-empty sentence or fall back to node text\n",
    "                        valid_sentences = [s for s in sentences if s and isinstance(s, str)]\n",
    "                        entity_sentences.append(valid_sentences[0] if valid_sentences else str(node))\n",
    "                        \n",
    "                # Log some debug information\n",
    "                logger.debug(f\"Processing {len(entity_nodes)} entities\")\n",
    "                logger.debug(f\"First few entity sentences: {entity_sentences[:3]}\")\n",
    "                \n",
    "                # Encode entity sentences\n",
    "                entity_embeddings = self.get_sentence_embeddings(entity_sentences)\n",
    "                \n",
    "                # Save unprojected embeddings\n",
    "                with open(entity_embeddings_path, 'wb') as f:\n",
    "                    pickle.dump((entity_nodes, entity_embeddings.cpu().numpy()), f)\n",
    "                logger.info(\"Saved entity embeddings to disk.\")\n",
    "                \n",
    "            # Convert to tensor and project\n",
    "            entity_embeddings = torch.tensor(entity_embeddings).to(device)\n",
    "            entity_embeddings = self.projection(entity_embeddings)\n",
    "            \n",
    "            # Move back to CPU for storage\n",
    "            entity_embeddings = entity_embeddings.cpu().detach().numpy()\n",
    "            \n",
    "            logger.debug(f\"Final entity embeddings shape: {entity_embeddings.shape}\")\n",
    "            return entity_nodes, entity_embeddings\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in entity embeddings computation: {str(e)}\")\n",
    "            logger.debug(traceback.format_exc())\n",
    "            # Return minimal valid output that won't break downstream processing\n",
    "            return [], np.zeros((0, 256))  # Empty embeddings with correct dimension\n",
    "\n",
    "    def get_sentence_window(self, center_idx: int, window_size: int = 7) -> List[str]:\n",
    "        \"\"\"Get a window of sentences around a central sentence with enhanced error handling.\"\"\"\n",
    "        try:\n",
    "            visited = set()\n",
    "            queue = [center_idx]\n",
    "            window = []\n",
    "            \n",
    "            # Validate center_idx\n",
    "            if not (0 <= center_idx < len(self.sentences)):\n",
    "                logger.warning(f\"Invalid center index {center_idx}, using index 0\")\n",
    "                center_idx = 0\n",
    "                queue = [0]\n",
    "                \n",
    "            while queue and len(window) < window_size:\n",
    "                current = queue.pop(0)\n",
    "                if current not in visited and 0 <= current < len(self.sentences):\n",
    "                    visited.add(current)\n",
    "                    window.append(self.sentences[current])\n",
    "                    \n",
    "                    # Get neighbors from sentence graph if they exist\n",
    "                    try:\n",
    "                        neighbors = list(self.sentence_graph.neighbors(current))\n",
    "                        queue.extend(neighbors)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error getting neighbors for index {current}: {str(e)}\")\n",
    "                        # Add adjacent sentences as fallback\n",
    "                        if current > 0:\n",
    "                            queue.append(current - 1)\n",
    "                        if current < len(self.sentences) - 1:\n",
    "                            queue.append(current + 1)\n",
    "                            \n",
    "            # If window is empty, take the direct context\n",
    "            if not window and 0 <= center_idx < len(self.sentences):\n",
    "                window = [self.sentences[center_idx]]\n",
    "                if center_idx > 0:\n",
    "                    window.insert(0, self.sentences[center_idx - 1])\n",
    "                if center_idx < len(self.sentences) - 1:\n",
    "                    window.append(self.sentences[center_idx + 1])\n",
    "                    \n",
    "            logger.debug(f\"Built sentence window with {len(window)} sentences\")\n",
    "            return window\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building sentence window: {str(e)}\")\n",
    "            # Return singleton window with the center sentence or first sentence as fallback\n",
    "            if 0 <= center_idx < len(self.sentences):\n",
    "                return [self.sentences[center_idx]]\n",
    "            return [self.sentences[0]]\n",
    "\n",
    "\n",
    "    def find_relevant_subgraph(self, query: str) -> Set[str]:\n",
    "        \"\"\"Find relevant nodes in the knowledge graph based on the query using embeddings and GNN reasoning.\"\"\"\n",
    "        try:\n",
    "            # Get query embedding and project it to 256 dimensions\n",
    "            query_embedding = self.encoder.encode([query], convert_to_tensor=True)\n",
    "            query_embedding = query_embedding.to(device)\n",
    "            \n",
    "            # Project query embedding and detach from computation graph\n",
    "            query_embedding = self.projection(query_embedding).detach().cpu().numpy()\n",
    "            \n",
    "            # Ensure entity embeddings are numpy array\n",
    "            if torch.is_tensor(self.entity_embeddings):\n",
    "                entity_embeddings = self.entity_embeddings.cpu().numpy()\n",
    "            else:\n",
    "                entity_embeddings = self.entity_embeddings\n",
    "            \n",
    "            # Calculate initial similarities\n",
    "            similarities = cosine_similarity(query_embedding, entity_embeddings)[0]\n",
    "            \n",
    "            # Set threshold and get relevant nodes\n",
    "            threshold = 0.3  # Lowered threshold for better recall\n",
    "            relevant_nodes = {\n",
    "                self.entity_nodes[i] \n",
    "                for i in range(len(self.entity_nodes)) \n",
    "                if similarities[i] > threshold\n",
    "            }\n",
    "            \n",
    "            # GNN reasoning to expand relevant nodes\n",
    "            try:\n",
    "                gnn_model = AdvancedGNNReasoner(\n",
    "                    num_node_features=self.gnn_data.num_node_features,\n",
    "                    num_relations=self.gnn_data.edge_type.max().item() + 1\n",
    "                ).to(device)\n",
    "                gnn_model.eval()\n",
    "                \n",
    "                # Load pre-trained GNN weights if available\n",
    "                gnn_weights_path = 'gnn_model.pth'\n",
    "                if os.path.exists(gnn_weights_path):\n",
    "                    gnn_model.load_state_dict(torch.load(gnn_weights_path, map_location=device))\n",
    "                    logger.debug(\"Loaded pre-trained GNN model weights.\")\n",
    "                else:\n",
    "                    logger.warning(\"GNN weights not found. Using randomly initialized weights.\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Get node embeddings from GNN\n",
    "                    node_embeddings = gnn_model(self.gnn_data)\n",
    "                    \n",
    "                    # Ensure proper shape and convert to numpy\n",
    "                    if torch.is_tensor(node_embeddings):\n",
    "                        # Reshape if needed (handling batch dimension)\n",
    "                        if len(node_embeddings.shape) == 3:\n",
    "                            node_embeddings = node_embeddings.squeeze(0)\n",
    "                        node_embeddings = node_embeddings.detach().cpu().numpy()\n",
    "                    \n",
    "                    # Reshape query embedding if needed\n",
    "                    query_embedding_reshaped = query_embedding.reshape(1, -1)\n",
    "                    \n",
    "                    # Compute similarities with proper shapes\n",
    "                    node_similarities = cosine_similarity(query_embedding_reshaped, node_embeddings)[0]\n",
    "                    \n",
    "                    gnn_threshold = 0.4  # Lowered threshold for better recall\n",
    "                    for i, sim in enumerate(node_similarities):\n",
    "                        if sim > gnn_threshold:\n",
    "                            node = self.idx_to_node.get(i)\n",
    "                            if node is not None:\n",
    "                                relevant_nodes.add(node)\n",
    "                        \n",
    "                    logger.debug(f\"GNN relevant nodes found: {len(relevant_nodes)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in GNN processing: {str(e)}\")\n",
    "                logger.debug(traceback.format_exc())\n",
    "                # Continue with just the semantic search results\n",
    "            \n",
    "            logger.debug(f\"Found {len(relevant_nodes)} relevant nodes\")\n",
    "            return relevant_nodes\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error finding relevant subgraph: {str(e)}\")\n",
    "            logger.debug(traceback.format_exc())\n",
    "            return set()  # Return empty set as fallback\n",
    "\n",
    "    def get_query_embedding(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Compute the embedding for the query.\"\"\"\n",
    "        query_embedding = self.encoder.encode([query], convert_to_tensor=True)\n",
    "        query_embedding = query_embedding.cpu().numpy()\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        # Project embedding if necessary\n",
    "        # query_embedding = self.projection(query_embedding)\n",
    "        return query_embedding[0]\n",
    "\n",
    "    def build_context(self, top_matches: List[Tuple[int, float]], relevant_nodes: Set[str]) -> List[str]:\n",
    "        \"\"\"Construct the context with enhanced error handling and fallback mechanisms.\"\"\"\n",
    "        try:\n",
    "            context_parts = []\n",
    "            \n",
    "            # Add context from semantic search\n",
    "            for idx, _ in top_matches:\n",
    "                try:\n",
    "                    window = self.get_sentence_window(idx)\n",
    "                    context_parts.extend(window)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error getting window for index {idx}: {str(e)}\")\n",
    "                    # Fallback: add single sentence if possible\n",
    "                    if 0 <= idx < len(self.sentences):\n",
    "                        context_parts.append(self.sentences[idx])\n",
    "\n",
    "            # Add context from knowledge graph\n",
    "            for node in relevant_nodes:\n",
    "                try:\n",
    "                    # Safely get connected sentences\n",
    "                    connected_sentences = []\n",
    "                    if node in self.knowledge_graph.nodes:\n",
    "                        connected_sentences = self.knowledge_graph.nodes[node].get('sentences', [])\n",
    "                        context_parts.extend(connected_sentences)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error getting connected sentences for node {node}: {str(e)}\")\n",
    "\n",
    "            # Ensure we have at least some context\n",
    "            if not context_parts and len(self.sentences) > 0:\n",
    "                logger.warning(\"No context found, using fallback\")\n",
    "                # Fallback: use first few sentences\n",
    "                context_parts = self.sentences[:3]\n",
    "\n",
    "            # Remove duplicates while preserving order\n",
    "            seen = set()\n",
    "            context = []\n",
    "            for part in context_parts:\n",
    "                if part not in seen:\n",
    "                    seen.add(part)\n",
    "                    context.append(part)\n",
    "\n",
    "            logger.debug(f\"Built context with {len(context)} unique sentences\")\n",
    "            return context\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building context: {str(e)}\")\n",
    "            # Ultimate fallback: return first sentence or empty list\n",
    "            return [self.sentences[0]] if len(self.sentences) > 0 else []\n",
    "\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self, openai_key: str):\n",
    "        self.client = OpenAI(api_key=openai_key)\n",
    "        self.logger = logging.getLogger(\"ASUSimRAG\")\n",
    "\n",
    "    def generate_answer(self, prompt: str) -> str:\n",
    "        \"\"\"Generate an answer using OpenAI's API with improved error handling.\"\"\"\n",
    "        try:\n",
    "            # Create chat completion with proper formatting\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4\",  # Use a more advanced model\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a highly knowledgeable assistant that provides accurate and reliable answers based on the provided context.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=1000,\n",
    "                temperature=0.2,  # Lower temperature for more deterministic responses\n",
    "                top_p=0.9,\n",
    "                frequency_penalty=0.5,\n",
    "                presence_penalty=0.0,\n",
    "            )\n",
    "\n",
    "            # Check response validity\n",
    "            if not response or not response.choices:\n",
    "                self.logger.error(\"OpenAI API returned empty response or no choices\")\n",
    "                return \"I apologize, but I cannot generate an answer at the moment due to a technical issue.\"\n",
    "\n",
    "            # Extract the message content\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            \n",
    "            if not answer:\n",
    "                self.logger.warning(\"Generated answer is empty\")\n",
    "                return \"I cannot provide an answer based on the available information.\"\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            self.logger.error(f\"Error in OpenAI API call: {error_msg}\")\n",
    "            self.logger.debug(traceback.format_exc())\n",
    "\n",
    "            # Handle specific error cases\n",
    "            if \"api_key\" in error_msg.lower():\n",
    "                return \"There was an issue with the API authentication. Please check your API key.\"\n",
    "            elif \"rate_limit\" in error_msg.lower():\n",
    "                return \"The service is currently experiencing high demand. Please try again in a moment.\"\n",
    "            elif \"invalid_request\" in error_msg.lower():\n",
    "                return \"There was an issue with the request format. Please try rephrasing your question.\"\n",
    "            else:\n",
    "                return \"I apologize, but I encountered an error while generating the answer. Please try again.\"\n",
    "\n",
    "    def validate_prompt(self, prompt: str) -> bool:\n",
    "        \"\"\"Validate the prompt before sending to the API.\"\"\"\n",
    "        if not prompt or not isinstance(prompt, str):\n",
    "            return False\n",
    "        if len(prompt.strip()) == 0:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "class RAGSystem:\n",
    "    # Lower the threshold to be more lenient with matches\n",
    "    SIMILARITY_THRESHOLD = 0.2  # Significantly lower threshold since we're using L2 distance normalization\n",
    "\n",
    "    def __init__(self, pdf_path: str, openai_key: str, ontology_path: str = None):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.openai_key = openai_key\n",
    "        self.ontology_path = ontology_path\n",
    "        self.text = None\n",
    "        self.sentences = None\n",
    "        self.processed_docs = None\n",
    "        self.knowledge_graph = None\n",
    "        self.embedding_manager = None\n",
    "        self.answer_generator = AnswerGenerator(openai_key=self.openai_key)\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.gnn_data = None\n",
    "        self.idx_to_node = None\n",
    "        self.node_to_idx = None\n",
    "        self.initialize_system()\n",
    "\n",
    "    def initialize_system(self):\n",
    "        \"\"\"Initialize all components of the RAG system.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.extract_text()\n",
    "        self.process_sentences()\n",
    "        self.build_knowledge_graph()\n",
    "        self.compute_embeddings()\n",
    "        self.initialize_gnn()\n",
    "        logger.info(f\"System initialized in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    def extract_text(self):\n",
    "        \"\"\"Extract or load text from PDF.\"\"\"\n",
    "        text_path = 'extracted_text.txt'\n",
    "        if os.path.exists(text_path):\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                self.text = f.read()\n",
    "            logger.info(\"Loaded extracted text from disk.\")\n",
    "        else:\n",
    "            extractor = PDFTextExtractor(self.pdf_path)\n",
    "            self.text = extractor.extract_text()\n",
    "            with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(self.text)\n",
    "            logger.info(\"Saved extracted text to disk.\")\n",
    "\n",
    "    def process_sentences(self):\n",
    "        \"\"\"Split and preprocess sentences.\"\"\"\n",
    "        sentences_path = 'sentences.pkl'\n",
    "        processed_docs_path = 'processed_docs.pkl'\n",
    "        if os.path.exists(sentences_path) and os.path.exists(processed_docs_path):\n",
    "            with open(sentences_path, 'rb') as f:\n",
    "                self.sentences = pickle.load(f)\n",
    "            with open(processed_docs_path, 'rb') as f:\n",
    "                self.processed_docs = pickle.load(f)\n",
    "            logger.info(\"Loaded sentences and processed docs from disk.\")\n",
    "        else:\n",
    "            processor = SentenceProcessor(self.text, self.nlp)\n",
    "            self.sentences = processor.split_into_sentences()\n",
    "            self.processed_docs = processor.preprocess_sentences()\n",
    "            with open(sentences_path, 'wb') as f:\n",
    "                pickle.dump(self.sentences, f)\n",
    "            with open(processed_docs_path, 'wb') as f:\n",
    "                pickle.dump(self.processed_docs, f)\n",
    "            logger.info(\"Saved sentences and processed docs to disk.\")\n",
    "\n",
    "    def build_knowledge_graph(self):\n",
    "        \"\"\"Build or load the knowledge graph.\"\"\"\n",
    "        kg_path = 'knowledge_graph.pkl'\n",
    "        mappings_path = 'node_mappings.pkl'\n",
    "        gnn_data_path = 'gnn_data.pt'\n",
    "        if os.path.exists(kg_path) and os.path.exists(mappings_path) and os.path.exists(gnn_data_path):\n",
    "            with open(kg_path, 'rb') as f:\n",
    "                self.knowledge_graph = pickle.load(f)\n",
    "            with open(mappings_path, 'rb') as f:\n",
    "                mappings = pickle.load(f)\n",
    "                self.idx_to_node = mappings['idx_to_node']\n",
    "                self.node_to_idx = mappings['node_to_idx']\n",
    "            # Load gnn_data\n",
    "            try:\n",
    "                self.gnn_data = torch.load(gnn_data_path, map_location=device)\n",
    "                logger.info(\"Loaded knowledge graph, node mappings, and gnn_data from disk.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading gnn_data: {e}\")\n",
    "                logger.debug(traceback.format_exc())\n",
    "                raise\n",
    "        else:\n",
    "            graph_builder = EnhancedKnowledgeGraphBuilder(self.processed_docs, self.sentences, self.ontology_path)\n",
    "            self.knowledge_graph = graph_builder.get_knowledge_graph()\n",
    "            self.gnn_data, self.idx_to_node, self.node_to_idx = graph_builder.convert_to_pyg_data()\n",
    "            # Save knowledge graph and node mappings\n",
    "            with open(kg_path, 'wb') as f:\n",
    "                pickle.dump(self.knowledge_graph, f)\n",
    "            with open(mappings_path, 'wb') as f:\n",
    "                pickle.dump({'idx_to_node': self.idx_to_node, 'node_to_idx': self.node_to_idx}, f)\n",
    "            # Save gnn_data\n",
    "            torch.save(self.gnn_data, gnn_data_path)\n",
    "            logger.info(\"Saved knowledge graph, node mappings, and gnn_data to disk.\")\n",
    "\n",
    "        # Consistency Checks\n",
    "        num_nodes_kg = len(self.knowledge_graph.nodes())\n",
    "        num_nodes_pyg = self.gnn_data.num_nodes if self.gnn_data is not None else None\n",
    "        num_nodes_mapping = len(self.idx_to_node)\n",
    "\n",
    "        if num_nodes_pyg != num_nodes_mapping or num_nodes_mapping != num_nodes_kg:\n",
    "            logger.error(f\"Node count mismatch: Knowledge Graph={num_nodes_kg}, PyG Data={num_nodes_pyg}, Mappings={num_nodes_mapping}\")\n",
    "            raise ValueError(\"Inconsistent node counts between knowledge graph, PyG data, and node mappings.\")\n",
    "        else:\n",
    "            logger.info(f\"Node counts are consistent: {num_nodes_kg} nodes.\")\n",
    "\n",
    "    def compute_embeddings(self):\n",
    "        \"\"\"Compute or load sentence embeddings.\"\"\"\n",
    "        self.embedding_manager = EmbeddingManager(self.sentences, embeddings_path='sentence_embeddings.pkl')\n",
    "\n",
    "    def initialize_gnn(self):\n",
    "        \"\"\"Initialize and train/load the GNN model.\"\"\"\n",
    "        try:\n",
    "            self.gnn_model = AdvancedGNNReasoner(\n",
    "                num_node_features=self.gnn_data.num_node_features, \n",
    "                num_relations=self.gnn_data.edge_type.max().item() + 1\n",
    "            ).to(device)\n",
    "            # Load pre-trained weights if available\n",
    "            gnn_weights_path = 'gnn_model.pth'\n",
    "            if os.path.exists(gnn_weights_path):\n",
    "                self.gnn_model.load_state_dict(torch.load(gnn_weights_path, map_location=device))\n",
    "                logger.info(\"Loaded pre-trained GNN model weights.\")\n",
    "            else:\n",
    "                logger.warning(\"GNN weights not found. Training from scratch is recommended for optimal performance.\")\n",
    "                # Implement training procedure if needed\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing GNN model: {e}\")\n",
    "            logger.debug(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "    def answer_question(self, query: str) -> str:\n",
    "        \"\"\"Generate an answer to the query using RAG with detailed debugging.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.logger = logging.getLogger(\"ASUSimRAG\")\n",
    "        \n",
    "        try:\n",
    "            # Input validation\n",
    "            if not query or not isinstance(query, str):\n",
    "                return \"Please provide a valid question.\"\n",
    "            \n",
    "            self.logger.debug(f\"Processing query: {query}\")\n",
    "            self.logger.debug(f\"Number of sentences in corpus: {len(self.sentences)}\")\n",
    "            \n",
    "            # Semantic search with error handling\n",
    "            searcher = SemanticSearcher(self.embedding_manager)\n",
    "            self.logger.debug(\"Initialized SemanticSearcher\")\n",
    "            \n",
    "            try:\n",
    "                top_matches = searcher.semantic_search(query, top_k=15)  # Increased from 5 to 10\n",
    "                self.logger.debug(f\"Semantic search returned {len(top_matches)} matches\")\n",
    "                \n",
    "                # Log the actual matches\n",
    "                for idx, score in top_matches:\n",
    "                    if 0 <= idx < len(self.sentences):\n",
    "                        self.logger.debug(f\"Match: index={idx}, score={score}, text={self.sentences[idx][:100]}...\")\n",
    "                    else:\n",
    "                        self.logger.error(f\"Invalid index {idx} in search results\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in semantic search: {str(e)}\")\n",
    "                return \"I encountered an error while searching for relevant information.\"\n",
    "\n",
    "            # Check if top match meets the similarity threshold\n",
    "            if not top_matches:\n",
    "                self.logger.info(\"No matches found in semantic search.\")\n",
    "                return \"I couldn't find any relevant information to answer your question.\"\n",
    "\n",
    "            top_match_score = top_matches[0][1]\n",
    "            self.logger.debug(f\"Top match similarity score: {top_match_score}\")\n",
    "\n",
    "            if top_match_score < self.SIMILARITY_THRESHOLD:\n",
    "                self.logger.info(f\"Top match score {top_match_score} below threshold {self.SIMILARITY_THRESHOLD}.\")\n",
    "                \n",
    "                # Instead of returning \"Answer not found\", try to use the matches we have\n",
    "                if top_match_score > 0.001:  # Very low threshold for any relevance\n",
    "                    self.logger.debug(\"Using available matches despite low similarity score\")\n",
    "                else:\n",
    "                    return \"I couldn't find sufficiently relevant information to answer your question.\"\n",
    "\n",
    "            # Context building with error checking\n",
    "            try:\n",
    "                self.logger.debug(\"Initializing ContextBuilder\")\n",
    "                context_builder = ContextBuilder(\n",
    "                    sentences=self.sentences,\n",
    "                    knowledge_graph=self.knowledge_graph,\n",
    "                    encoder=self.embedding_manager.text_model,\n",
    "                    processed_docs=self.processed_docs,\n",
    "                    gnn_data=self.gnn_data,\n",
    "                    idx_to_node=self.idx_to_node,\n",
    "                    node_to_idx=self.node_to_idx\n",
    "                )\n",
    "                \n",
    "                # Find relevant nodes using both semantic search and GNN reasoning\n",
    "                self.logger.debug(\"Finding relevant subgraph\")\n",
    "                relevant_nodes = context_builder.find_relevant_subgraph(query)\n",
    "                self.logger.debug(f\"Found {len(relevant_nodes)} relevant nodes\")\n",
    "                \n",
    "                # Build context\n",
    "                self.logger.debug(\"Building context\")\n",
    "                context = context_builder.build_context(top_matches, relevant_nodes)\n",
    "                self.logger.debug(f\"Built context with {len(context)} sentences\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in context building: {str(e)}\")\n",
    "                self.logger.debug(traceback.format_exc())\n",
    "                \n",
    "                # Fallback: use direct matches if available\n",
    "                self.logger.debug(\"Attempting to use fallback context\")\n",
    "                context = []\n",
    "                for idx, _ in top_matches:\n",
    "                    if 0 <= idx < len(self.sentences):\n",
    "                        context.append(self.sentences[idx])\n",
    "                \n",
    "                if not context:\n",
    "                    return \"I encountered an error while building context for your question.\"\n",
    "\n",
    "            # Process and limit context\n",
    "            max_context_length = 4000  # Increased from 3000\n",
    "            context_str = ' '.join(context)\n",
    "            if len(context_str) > max_context_length:\n",
    "                words = context_str.split()\n",
    "                truncated_words = words[:max_context_length // 10]\n",
    "                context_str = ' '.join(truncated_words)\n",
    "                self.logger.debug(f\"Truncated context to {len(context_str)} characters\")\n",
    "\n",
    "            # Build prompt with more detailed instructions\n",
    "            prompt = f\"\"\"Use the following context information to answer the question accurately and comprehensively. If the information in the context is insufficient for a complete answer, focus on what can be confidently stated from the available information.\n",
    "\n",
    "Context:\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a detailed answer that:\n",
    "1. Directly addresses the key points in the question\n",
    "2. Includes specific requirements or eligibility criteria if mentioned\n",
    "3. References any relevant deadlines or processes\n",
    "4. Notes any additional resources or next steps\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "            # Generate answer with error handling\n",
    "            try:\n",
    "                self.logger.debug(\"Generating answer using OpenAI\")\n",
    "                answer = self.answer_generator.generate_answer(prompt)\n",
    "                logger.info(f\"Answer generated in {time.time() - start_time:.2f} seconds\")\n",
    "                return answer\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error generating answer: {str(e)}\")\n",
    "                return \"I encountered an error while generating the answer. Please try again.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in RAG pipeline: {str(e)}\")\n",
    "            self.logger.debug(traceback.format_exc())\n",
    "            return \"I encountered an error while processing your question. Please try again.\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,  # Temporarily set to DEBUG for more detailed output\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(\"ASUSimRAG\")\n",
    "\n",
    "    try:\n",
    "        # Initialize the RAG system\n",
    "        logger.info(\"Initializing RAG system...\")\n",
    "        rag = RAGSystem(\n",
    "            pdf_path=\"/Users/rohit/Desktop/ASU/Finances.pdf\",\n",
    "            openai_key=\"\",  # Replace with your actual OpenAI API key\n",
    "            ontology_path=\"\"  # Replace with the path to your ontology if available\n",
    "        )\n",
    "        logger.info(\"RAG system initialized successfully\")\n",
    "\n",
    "        # Example questions\n",
    "        questions = [\n",
    "            \"What are the options and requirements for obtaining scholarships and awards as a transfer student at ASU?\"\n",
    "        ]\n",
    "\n",
    "        # Get answers\n",
    "        for question in questions:\n",
    "            logger.info(f\"Processing question: {question}\")\n",
    "            try:\n",
    "                answer = rag.answer_question(question)\n",
    "                print(f\"\\nQuestion: {question}\")\n",
    "                print(f\"Answer: {answer}\")\n",
    "                logger.info(\"Successfully generated answer\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing question: {str(e)}\")\n",
    "                print(f\"Error: Unable to process question - {str(e)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {e}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juspay39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

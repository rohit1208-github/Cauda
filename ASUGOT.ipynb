{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 09:19:00,924 - INFO - Initializing DataLoader\n",
      "2024-11-12 09:19:00,926 - INFO - Loading PDF file: /Users/rohit/Desktop/ASU/Finances.pdf\n",
      "2024-11-12 09:19:06,020 - INFO - Successfully loaded and extracted text from /Users/rohit/Desktop/ASU/Finances.pdf\n",
      "2024-11-12 09:19:06,021 - INFO - Loading CSV tables\n",
      "2024-11-12 09:19:06,021 - INFO - Loading CSV file: /Users/rohit/Desktop/ASU/Student_Financial_Aid_and_Application_Information.csv\n",
      "2024-11-12 09:19:06,023 - INFO - Successfully loaded /Users/rohit/Desktop/ASU/Student_Financial_Aid_and_Application_Information.csv\n",
      "2024-11-12 09:19:06,024 - INFO - Loading CSV file: /Users/rohit/Desktop/ASU/Scholarships__Grants__and_Payment_Options.csv\n",
      "2024-11-12 09:19:06,025 - INFO - Successfully loaded /Users/rohit/Desktop/ASU/Scholarships__Grants__and_Payment_Options.csv\n",
      "2024-11-12 09:19:06,025 - INFO - Loading CSV file: /Users/rohit/Desktop/ASU/Employment__Loans__and_Additional_Financial_Resources.csv\n",
      "2024-11-12 09:19:06,026 - INFO - Successfully loaded /Users/rohit/Desktop/ASU/Employment__Loans__and_Additional_Financial_Resources.csv\n",
      "2024-11-12 09:19:06,027 - INFO - Loading embeddings from embeddings_cache/pdf_embeddings.pkl\n",
      "2024-11-12 09:19:06,029 - INFO - PDF embeddings loaded from file successfully.\n",
      "2024-11-12 09:19:06,029 - INFO - Starting analysis for question: Can I apply financial aid\n",
      "2024-11-12 09:19:06,029 - INFO - Starting analysis\n",
      "2024-11-12 09:19:06,030 - INFO - Expanding node: Analyze the question: Can I apply financial aid at depth 0\n",
      "2024-11-12 09:19:06,030 - INFO - Searching PDF for query: Analyze the question: Can I apply financial aid...\n",
      "2024-11-12 09:19:06,030 - INFO - Generating embedding for text: Analyze the question: Can I apply financial aid...\n",
      "2024-11-12 09:19:06,809 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:06,915 - INFO - Embedding generated successfully\n",
      "2024-11-12 09:19:06,923 - INFO - Found 5 relevant PDF texts.\n",
      "2024-11-12 09:19:08,137 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:08,139 - INFO - Generated pandas code: result = pd.concat([df1[df1['Student_ID'] == 3], df2[df2['Student_ID'] == 3], df3[df3['Student_ID'] == 3]])\n",
      "2024-11-12 09:19:08,144 - INFO - Checking if answer is possible for node: Analyze the question: Can I apply financial aid\n",
      "2024-11-12 09:19:10,086 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:10,092 - INFO - LLM response for answer possibility:\n",
      "Answer: YES\n",
      "Confidence: 0.9\n",
      "Justification: The fetched PDF information and CSV data provide information on eligibility for financial aid, including federal student aid, institutional grants, and scholarships. The information also mentions options for additional funding, such as payment plans, student employment, and private loans. However, the information does not specify the specific criteria for eligibility or the amount of aid that may be awarded, so there may be some uncertainty.\n",
      "2024-11-12 09:19:10,093 - INFO - Answer criteria met with sufficient confidence for this node.\n",
      "2024-11-12 09:19:11,313 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:11,315 - INFO - Added child node: Consider alternative options for funding, such as private loans or payment plans.\n",
      "2024-11-12 09:19:11,315 - INFO - Added child node: Check if the student's financial need level is considered high.\n",
      "2024-11-12 09:19:11,316 - INFO - Added child node: Review the student's previous financial aid package and determine if a reevaluation is possible.\n",
      "2024-11-12 09:19:11,316 - INFO - Iteration 1: Expanded node 'Analyze the question: Can I apply financial aid' with Q-value 1.0\n",
      "2024-11-12 09:19:11,317 - INFO - Updated Q-value for node 'Analyze the question: Can I apply financial aid': 1.0\n",
      "2024-11-12 09:19:11,317 - INFO - New best Q-value found: 1.0\n",
      "2024-11-12 09:19:11,317 - INFO - Expanding node: Consider alternative options for funding, such as private loans or payment plans. at depth 1\n",
      "2024-11-12 09:19:11,318 - INFO - Searching PDF for query: Consider alternative options for funding, such as ...\n",
      "2024-11-12 09:19:11,318 - INFO - Generating embedding for text: Consider alternative options for funding, such as ...\n",
      "2024-11-12 09:19:11,620 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:11,728 - INFO - Embedding generated successfully\n",
      "2024-11-12 09:19:11,743 - INFO - Found 5 relevant PDF texts.\n",
      "2024-11-12 09:19:12,950 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:12,952 - INFO - Generated pandas code: result = pd.concat([df1[df1['Student_ID'] == 3], df2[df2['Student_ID'] == 3], df3[df3['Student_ID'] == 3]])\n",
      "2024-11-12 09:19:12,956 - INFO - Checking if answer is possible for node: Consider alternative options for funding, such as private loans or payment plans.\n",
      "2024-11-12 09:19:14,802 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:14,805 - INFO - LLM response for answer possibility:\n",
      "Answer: YES\n",
      "Confidence: 0.8\n",
      "Justification: The current thought mentions considering alternative options for funding, such as private loans, which suggests that financial aid may not be available. The fetched PDF information also mentions that federal graduate PLUS loans are subject to credit approval and may be denied, and that private education loans are subject to credit approval. The fetched CSV data shows that the student has applied for financial aid and has a high financial need level, but it is unclear if they have been awarded any scholarships or loans. Overall, there is enough information to suggest that the student may be able to apply for financial aid, but it is not certain if they will be approved or receive a sufficient amount.\n",
      "2024-11-12 09:19:14,806 - INFO - Answer criteria met with sufficient confidence for this node.\n",
      "2024-11-12 09:19:16,228 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:16,231 - INFO - Added child node: Consider the option of applying for a Parent PLUS Loan.\n",
      "2024-11-12 09:19:16,232 - INFO - Added child node: Discuss the possibility of the student obtaining a private loan.\n",
      "2024-11-12 09:19:16,232 - INFO - Added child node: Suggest contacting the financial aid office for further assistance and guidance.\n",
      "2024-11-12 09:19:16,233 - INFO - Iteration 2: Expanded node 'Consider alternative options for funding, such as private loans or payment plans.' with Q-value 1.0\n",
      "2024-11-12 09:19:16,234 - INFO - Updated Q-value for node 'Consider alternative options for funding, such as private loans or payment plans.': 4.84\n",
      "2024-11-12 09:19:16,234 - INFO - New best Q-value found: 4.84\n",
      "2024-11-12 09:19:16,235 - INFO - Expanding node: Check if the student's financial need level is considered high. at depth 1\n",
      "2024-11-12 09:19:16,235 - INFO - Searching PDF for query: Check if the student's financial need level is con...\n",
      "2024-11-12 09:19:16,236 - INFO - Generating embedding for text: Check if the student's financial need level is con...\n",
      "2024-11-12 09:19:16,639 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:16,748 - INFO - Embedding generated successfully\n",
      "2024-11-12 09:19:16,760 - INFO - Found 5 relevant PDF texts.\n",
      "2024-11-12 09:19:17,866 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:17,870 - INFO - Generated pandas code: result = pd.concat([df1[df1['Student_ID'] == 3], df2[df2['Student_ID'] == 3], df3[df3['Student_ID'] == 3]])\n",
      "result[result['Financial_Need_Level'] == 'High']\n",
      "2024-11-12 09:19:17,877 - INFO - Checking if answer is possible for node: Check if the student's financial need level is considered high.\n",
      "2024-11-12 09:19:19,812 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:19,817 - INFO - LLM response for answer possibility:\n",
      "Answer: YES\n",
      "Confidence: 0.8\n",
      "Justification: The current thought mentions checking the student's financial need level, and the fetched PDF information provides information on the ASU College Attainment Grant Program, which covers tuition and fees for low-income families. The fetched CSV data also includes information on financial aid amount and scholarship awarded, indicating that the student may be eligible for financial aid. However, more information may be needed to determine the specific eligibility requirements for financial aid at ASU.\n",
      "2024-11-12 09:19:19,818 - INFO - Answer criteria met with sufficient confidence for this node.\n",
      "2024-11-12 09:19:21,041 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:21,046 - INFO - Added child node: Consider alternative options for financial aid, such as private loans or student employment.\n",
      "2024-11-12 09:19:21,047 - INFO - Added child node: Determine if the student's financial need level is considered high enough to qualify for financial aid.\n",
      "2024-11-12 09:19:21,047 - INFO - Added child node: Advise the student to contact the financial aid office for further assistance and guidance.\n",
      "2024-11-12 09:19:21,048 - INFO - Iteration 3: Expanded node 'Check if the student's financial need level is considered high.' with Q-value 1.0\n",
      "2024-11-12 09:19:21,049 - INFO - Updated Q-value for node 'Check if the student's financial need level is considered high.': 4.84\n",
      "2024-11-12 09:19:21,049 - INFO - No improvement in Q-value. Iterations without improvement: 1\n",
      "2024-11-12 09:19:21,050 - INFO - Answer found; terminating analysis early.\n",
      "2024-11-12 09:19:21,050 - INFO - Generating answer for node: Check if the student's financial need level is considered high.\n",
      "2024-11-12 09:19:25,546 - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 09:19:25,550 - INFO - Answer generated: Based on the information provided, it appears that Student ID 3, Taylor Brown, has applied for financial aid and has been approved for a scholarship. However, it is important to note that the scholarship amount and type are not specified in the data. \n",
      "\n",
      "In order to determine if Taylor is eligible for financial aid, we need to consider their financial need level. According to the data, Taylor's financial need level is not specified. However, we can assume that they have a high financial need level since they have applied for financial aid and have been approved for a scholarship. \n",
      "\n",
      "In order to confirm their eligibility for financial aid, Taylor should check if they meet the criteria for a high financial need level. This can be determined by answering the following questions: Are you a veteran of the U.S. armed forces? Are you currently serving on active duty in the U.S. armed forces for purposes other than training? Do you have children who receive more than half of their financial support from you? Do you have dependents, other than a spouse or children, who live with you and receive more than half of their financial support from you? Did a court make you an emancipated minor before the age of 18? Did a court place you under legal guardianship? \n",
      "\n",
      "If Taylor answers yes to any of these questions, they may be considered to have a high financial need level and may be eligible for additional financial aid. It is important for Taylor to provide this information to the financial aid office in order to determine their eligibility for financial aid. \n",
      "\n",
      "In addition, Taylor should also check if they have met the priority filing date for FAFSA submission. This information is not specified in the data, but it is important to meet the priority filing date in order to receive the maximum amount of financial aid. \n",
      "\n",
      "In conclusion, based on the information provided, it appears that Taylor Brown may be eligible for financial aid. However, it is important for them to provide more information to the financial aid office in order to determine their eligibility and receive the maximum amount of financial aid. It is also important for Taylor to meet the priority filing date for FAFSA submission in order to receive the maximum amount of financial aid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to your question:\n",
      "Based on the information provided, it appears that Student ID 3, Taylor Brown, has applied for financial aid and has been approved for a scholarship. However, it is important to note that the scholarship amount and type are not specified in the data. \n",
      "\n",
      "In order to determine if Taylor is eligible for financial aid, we need to consider their financial need level. According to the data, Taylor's financial need level is not specified. However, we can assume that they have a high financial need level since they have applied for financial aid and have been approved for a scholarship. \n",
      "\n",
      "In order to confirm their eligibility for financial aid, Taylor should check if they meet the criteria for a high financial need level. This can be determined by answering the following questions: Are you a veteran of the U.S. armed forces? Are you currently serving on active duty in the U.S. armed forces for purposes other than training? Do you have children who receive more than half of their financial support from you? Do you have dependents, other than a spouse or children, who live with you and receive more than half of their financial support from you? Did a court make you an emancipated minor before the age of 18? Did a court place you under legal guardianship? \n",
      "\n",
      "If Taylor answers yes to any of these questions, they may be considered to have a high financial need level and may be eligible for additional financial aid. It is important for Taylor to provide this information to the financial aid office in order to determine their eligibility for financial aid. \n",
      "\n",
      "In addition, Taylor should also check if they have met the priority filing date for FAFSA submission. This information is not specified in the data, but it is important to meet the priority filing date in order to receive the maximum amount of financial aid. \n",
      "\n",
      "In conclusion, based on the information provided, it appears that Taylor Brown may be eligible for financial aid. However, it is important for them to provide more information to the financial aid office in order to determine their eligibility and receive the maximum amount of financial aid. It is also important for Taylor to meet the priority filing date for FAFSA submission in order to receive the maximum amount of financial aid.\n",
      "\n",
      "Traversed Path:\n",
      "- Analyze the question: Can I apply financial aid\n",
      "- Check if the student's financial need level is considered high.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import logging\n",
    "import tiktoken\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential, RetryError\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import signal\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import random\n",
    "from queue import PriorityQueue\n",
    "import re\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "from functools import lru_cache\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pypdf  # Updated import\n",
    "import ast  # New import for syntax checking\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"analysis.log\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key='')  # Please insert your OpenAI API key here\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        logging.info(\"Initializing DataLoader\")\n",
    "        self.pdf_texts = self.load_pdf('/Users/rohit/Desktop/ASU/Finances.pdf')\n",
    "        self.csv_tables = self.load_csv_tables({\n",
    "            'Student_Financial_Aid_and_Application_Information.csv': '/Users/rohit/Desktop/ASU/Student_Financial_Aid_and_Application_Information.csv',\n",
    "            'Scholarships__Grants__and_Payment_Options.csv': '/Users/rohit/Desktop/ASU/Scholarships__Grants__and_Payment_Options.csv',\n",
    "            'Employment__Loans__and_Additional_Financial_Resources.csv': '/Users/rohit/Desktop/ASU/Employment__Loans__and_Additional_Financial_Resources.csv',\n",
    "        })\n",
    "\n",
    "    def load_pdf(self, pdf_path):\n",
    "        logging.info(f\"Loading PDF file: {pdf_path}\")\n",
    "        texts = []\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as f:\n",
    "                reader = pypdf.PdfReader(f)\n",
    "                for page in reader.pages:\n",
    "                    texts.append(page.extract_text())\n",
    "            logging.info(f\"Successfully loaded and extracted text from {pdf_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading PDF file {pdf_path}: {e}\")\n",
    "        return texts\n",
    "\n",
    "    def load_csv_tables(self, csv_paths):\n",
    "        logging.info(\"Loading CSV tables\")\n",
    "        tables = {}\n",
    "        for table_name, csv_path in csv_paths.items():\n",
    "            logging.info(f\"Loading CSV file: {csv_path}\")\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                tables[table_name] = df\n",
    "                logging.info(f\"Successfully loaded {csv_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading CSV file {csv_path}: {e}\")\n",
    "        return tables\n",
    "\n",
    "class VectorSearch:\n",
    "    def __init__(self, embedding_model=\"text-embedding-ada-002\"):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embedding_dim = 1536 if \"ada-002\" in embedding_model else 768\n",
    "        self.pdf_embeddings = None\n",
    "        self.pdf_texts = []\n",
    "        self.pdf_nn = None\n",
    "        self.embeddings_dir = 'embeddings_cache'\n",
    "        if not os.path.exists(self.embeddings_dir):\n",
    "            os.makedirs(self.embeddings_dir)\n",
    "        # Tokenizer for the specific model\n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        return len(self.encoding.encode(text))\n",
    "\n",
    "    def load_embeddings(self, file_name):\n",
    "        \"\"\"Load embeddings from file if it exists.\"\"\"\n",
    "        file_path = os.path.join(self.embeddings_dir, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            logging.info(f\"Loading embeddings from {file_path}\")\n",
    "            with open(file_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            logging.info(f\"Embeddings file {file_path} not found.\")\n",
    "            return None\n",
    "\n",
    "    def save_embeddings(self, file_name, embeddings_data):\n",
    "        \"\"\"Save embeddings to file.\"\"\"\n",
    "        file_path = os.path.join(self.embeddings_dir, file_name)\n",
    "        logging.info(f\"Saving embeddings to {file_path}\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(embeddings_data, f)\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_random_exponential(min=1, max=10))\n",
    "    def generate_embedding(self, text):\n",
    "        try:\n",
    "            logging.info(f\"Generating embedding for text: {text[:50]}...\")\n",
    "            response = client.embeddings.create(\n",
    "                input=text,\n",
    "                model=self.embedding_model\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            time.sleep(0.1)  # Small delay between requests\n",
    "            logging.info(\"Embedding generated successfully\")\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate embedding for text: {text[:50]}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_or_build_pdf_vectors(self, pdf_texts):\n",
    "        embeddings_file = 'pdf_embeddings.pkl'\n",
    "        # Load previously saved embeddings if available\n",
    "        saved_data = self.load_embeddings(embeddings_file)\n",
    "        if saved_data:\n",
    "            self.pdf_embeddings, self.pdf_texts = saved_data\n",
    "            self.pdf_nn = NearestNeighbors(n_neighbors=5, metric='cosine').fit(self.pdf_embeddings)\n",
    "            logging.info(\"PDF embeddings loaded from file successfully.\")\n",
    "            return\n",
    "        # Otherwise, build embeddings\n",
    "        logging.info(\"Building embeddings for PDF texts\")\n",
    "        embeddings = []\n",
    "        for text in tqdm(pdf_texts, desc=\"Generating embeddings for PDF\"):\n",
    "            try:\n",
    "                embedding = self.generate_embedding(text)\n",
    "                embeddings.append(embedding)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error generating embedding for PDF text: {e}\")\n",
    "                embeddings.append([0.0]*self.embedding_dim)\n",
    "        self.pdf_embeddings = np.array(embeddings)\n",
    "        self.pdf_texts = pdf_texts\n",
    "        self.pdf_nn = NearestNeighbors(n_neighbors=5, metric='cosine').fit(self.pdf_embeddings)\n",
    "        self.save_embeddings(embeddings_file, (self.pdf_embeddings, self.pdf_texts))\n",
    "        logging.info(\"PDF embeddings built and saved successfully.\")\n",
    "\n",
    "    def search_pdf(self, query, top_k=5):\n",
    "        logging.info(f\"Searching PDF for query: {query[:50]}...\")\n",
    "        try:\n",
    "            query_embedding = self.generate_embedding(query)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating embedding for query: {e}\")\n",
    "            return []\n",
    "        if query_embedding is None:\n",
    "            logging.warning(\"Failed to generate embedding for query. Returning empty list.\")\n",
    "            return []\n",
    "        query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "        distances, indices = self.pdf_nn.kneighbors(query_embedding, n_neighbors=top_k)\n",
    "        relevant_texts = [self.pdf_texts[idx] for idx in indices[0]]\n",
    "        logging.info(f\"Found {len(relevant_texts)} relevant PDF texts.\")\n",
    "        return relevant_texts\n",
    "\n",
    "class ExecutorModule:\n",
    "    def __init__(self, data_loader, vector_search, student_id):\n",
    "        self.data_loader = data_loader\n",
    "        self.vector_search = vector_search\n",
    "        self.student_id = student_id  # Store the student ID\n",
    "\n",
    "    def fetch_pdf_chunks(self, query):\n",
    "        return self.vector_search.search_pdf(query)\n",
    "\n",
    "    def fetch_csv_data(self, query):\n",
    "        # Use LLM to generate a query to fetch data from CSV files\n",
    "        prompt = f\"\"\"\n",
    "You are an expert data assistant. Write a pandas code snippet to extract data for Student ID {self.student_id} from the appropriate DataFrames for the following requirement:\n",
    "\n",
    "Requirement: {query}\n",
    "\n",
    "Available DataFrames:\n",
    "\n",
    "- df1: Student_Financial_Aid_and_Application_Information.csv\n",
    "  Columns: Student_ID, Name, Financial_Aid_Applied, FAFSA_Submitted, Priority_Filing_Date_Met, Financial_Need_Level, Aid_Package_Reviewed, Financial_Aid_Amount\n",
    "\n",
    "- df2: Scholarships__Grants__and_Payment_Options.csv\n",
    "  Columns: Student_ID, Scholarship_Awarded, Scholarship_Type, Scholarship_Amount, Institutional_Grant, ASU_Payment_Plan, Summer_Financial_Aid_Eligible, Private_Loan_Required\n",
    "\n",
    "- df3: Employment__Loans__and_Additional_Financial_Resources.csv\n",
    "  Columns: Student_ID, Work_Study_Eligible, Work_Study_Accepted, Parent_PLUS_Loan, Parent_Plus_Loan_Status, Private_Loan, Student_Employment, Alternative_Options\n",
    "\n",
    "Note: The DataFrames are preloaded and named as df1, df2, and df3. Use these variable names in your code.\n",
    "\n",
    "Important:\n",
    "\n",
    "- Use `pd.concat` instead of `append` to combine DataFrames.\n",
    "- Ensure that the final DataFrame is assigned to a variable named 'result' and that it only contains data for Student ID {self.student_id}.\n",
    "- Provide only the pandas code to execute the query. Do not include explanations.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=500,  # Increased max_tokens\n",
    "                temperature=0,\n",
    "            )\n",
    "            code = response.choices[0].text.strip()\n",
    "            logging.info(f\"Generated pandas code: {code}\")\n",
    "\n",
    "            # Check if code is syntactically correct\n",
    "            try:\n",
    "                ast.parse(code)\n",
    "            except SyntaxError as se:\n",
    "                logging.error(f\"Syntax error in generated code: {se}\")\n",
    "                return None\n",
    "\n",
    "            # Execute the code safely\n",
    "            local_vars = {}\n",
    "            # Add the dataframes to local_vars with specified variable names\n",
    "            local_vars['df1'] = self.data_loader.csv_tables['Student_Financial_Aid_and_Application_Information.csv']\n",
    "            local_vars['df2'] = self.data_loader.csv_tables['Scholarships__Grants__and_Payment_Options.csv']\n",
    "            local_vars['df3'] = self.data_loader.csv_tables['Employment__Loans__and_Additional_Financial_Resources.csv']\n",
    "\n",
    "            # Replace 'append' with 'pd.concat' in the code if necessary\n",
    "            if 'append' in code:\n",
    "                logging.info(\"Replacing 'append' with 'pd.concat' in the generated code.\")\n",
    "                # This is a simple replacement and may need to be adjusted based on the actual code\n",
    "                code = code.replace('.append(', ', ').replace('append(', 'pd.concat([')\n",
    "                code = code.replace(')', '])', 1)  # Replace only the first closing parenthesis\n",
    "\n",
    "            # Execute the code\n",
    "            exec(code, {\"pd\": pd}, local_vars)\n",
    "            result = local_vars.get('result', None)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating or executing pandas query: {e}\")\n",
    "            return None\n",
    "\n",
    "class ThoughtNode:\n",
    "    def __init__(self, description, parent=None, depth=0):\n",
    "        self.description = description\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.fetched_pdf = None\n",
    "        self.fetched_csv_data = None\n",
    "        self.q_value = 1.0  # Optimistically initialized Q-value\n",
    "        self.target_q_value = 1.0  # Target Q-value for stabilization\n",
    "        self.heuristic = 0\n",
    "        self.depth = depth\n",
    "        self.visits = 0\n",
    "        self.answer_possible = None  # New attribute to store answer possibility\n",
    "        self.answer_confidence = 0   # Confidence level for answer possibility\n",
    "        self.timestamp = datetime.now()  # Added for temporal rewards\n",
    "        self.node_type = 'regular'  # New attribute for node type\n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def get_unique_id(self):\n",
    "        path = []\n",
    "        current = self\n",
    "        while current:\n",
    "            path.append(current.description)\n",
    "            current = current.parent\n",
    "        return '->'.join(reversed(path))\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return (self.q_value + self.heuristic) > (other.q_value + other.heuristic)\n",
    "\n",
    "class GraphOfThoughts:\n",
    "    def __init__(self, question, data_loader, vector_search, student_id, config=None):\n",
    "        logging.info(f\"Starting analysis for question: {question}\")\n",
    "        self.question = question\n",
    "        self.data_loader = data_loader\n",
    "        self.vector_search = vector_search\n",
    "        self.student_id = student_id  # Store the student ID\n",
    "        self.executor_module = ExecutorModule(data_loader, vector_search, student_id)\n",
    "        self.visited_nodes = set()\n",
    "        self.queue = PriorityQueue()\n",
    "        self.iteration_limit = config.get('iteration_limit', 1000) if config else 1000\n",
    "        self.iteration_limit_without_improvement = config.get('iteration_limit_without_improvement', 100) if config else 100\n",
    "        self.iterations = 0\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.root = ThoughtNode(f\"Analyze the question: {question}\", depth=0)\n",
    "        self.root.node_type = 'root'  # Set root node type\n",
    "        self.graph.add_node(self.root.description, node=self.root)\n",
    "        self.pos = None\n",
    "        self.traversed_path = []\n",
    "        self.recurring_missing_info = set()\n",
    "        self.recurring_threshold = config.get('recurring_threshold', 3) if config else 3\n",
    "        self.max_iterations = config.get('max_iterations', 800) if config else 800\n",
    "        self.max_depth = config.get('max_depth', 25) if config else 25\n",
    "        self.min_exploration_depth = config.get('min_exploration_depth', 3) if config else 3\n",
    "        self.answer_confidence_threshold = config.get('answer_confidence_threshold', 0.7) if config else 0.7\n",
    "        self.convergence_threshold = config.get('convergence_threshold', 0.005) if config else 0.005\n",
    "        self.breadth = config.get('breadth', 7) if config else 7\n",
    "        self.visualization_interval = config.get('visualization_interval', 15) if config else 15\n",
    "\n",
    "        # Initialize dynamic parameters\n",
    "        self.alpha = 0.3  # Initial learning rate\n",
    "        self.gamma = 0.8  # Initial discount factor\n",
    "        self.epsilon = 0.7  # Increased from 0.5 to 0.7 for more exploration\n",
    "        self.epsilon_min = 0.1  # Minimum epsilon value\n",
    "        self.epsilon_decay_factor = 0.995  # Slower decay\n",
    "\n",
    "        # Weights for priority calculation\n",
    "        self.q_weight = 0.7  # Increased weight for Q-value\n",
    "        self.h_weight = 0.3  # Decreased weight for heuristic\n",
    "\n",
    "        # For summary generation\n",
    "        self.analysis_steps = []\n",
    "\n",
    "        # Initialize a lock for thread-safe operations\n",
    "        self.lock = Lock()\n",
    "\n",
    "        # Initialize answer cache to reduce redundant API calls\n",
    "        self.answer_cache = {}\n",
    "\n",
    "        # To store multiple answer findings\n",
    "        self.answer_findings = []\n",
    "\n",
    "        # Experience replay buffer\n",
    "        self.experience_replay_buffer = []\n",
    "        self.replay_buffer_size = config.get('replay_buffer_size', 100)\n",
    "\n",
    "        # UCB exploration parameter\n",
    "        self.c = config.get('exploration_constant', 1.9) if config else 1.9\n",
    "\n",
    "        # For dynamic parameter adjustment\n",
    "        self.iterations_without_improvement = 0\n",
    "        self.performance_history = []\n",
    "\n",
    "        # Initialize high-value paths\n",
    "        self.high_value_paths = set()\n",
    "        self.explored_thoughts = set()\n",
    "\n",
    "        # Enhanced: Initialize a set to track timestamps for temporal rewards\n",
    "        self.node_timestamps = {}\n",
    "\n",
    "        # Initialize target network parameters\n",
    "        self.target_q_update_frequency = config.get('target_q_update_frequency', 100) if config else 100\n",
    "        self.iterations_since_target_update = 0  # Counter to track when to update target network\n",
    "\n",
    "    def compute_priority(self, node):\n",
    "        # Combine q_value and heuristic with weights\n",
    "        return -(self.q_weight * node.q_value + self.h_weight * node.heuristic)\n",
    "\n",
    "    def calculate_heuristic(self, node):\n",
    "        with self.lock:\n",
    "            # Calculate relevance scores\n",
    "            data_relevance = len(node.fetched_pdf) if node.fetched_pdf else 0\n",
    "            csv_relevance = len(node.fetched_csv_data) if node.fetched_csv_data is not None else 0\n",
    "\n",
    "        # Depth Factor: Encourage deeper exploration but penalize excessive depth\n",
    "        depth_penalty = (node.depth / self.max_depth) ** 2  # Quadratic penalty for depth\n",
    "\n",
    "        # Include a factor for unexplored nodes\n",
    "        if node.get_unique_id() not in self.visited_nodes:\n",
    "            unexplored_bonus = 2.0\n",
    "        else:\n",
    "            unexplored_bonus = 0.0\n",
    "\n",
    "        # Adjusted Heuristic Calculation\n",
    "        heuristic = (data_relevance + csv_relevance) * (1 - depth_penalty) + unexplored_bonus\n",
    "\n",
    "        return heuristic\n",
    "\n",
    "    def update_q_value(self, node, reward):\n",
    "        # Store experience\n",
    "        self.experience_replay_buffer.append((node, reward))\n",
    "        if len(self.experience_replay_buffer) > self.replay_buffer_size:\n",
    "            self.experience_replay_buffer.pop(0)\n",
    "\n",
    "        # Sample from experience replay buffer\n",
    "        experiences = random.sample(self.experience_replay_buffer, min(len(self.experience_replay_buffer), 10))\n",
    "        for exp_node, exp_reward in experiences:\n",
    "            if exp_node.parent:\n",
    "                # Use target_q_value for max_child_q\n",
    "                max_child_q = max([child.target_q_value for child in exp_node.children], default=0)\n",
    "                exp_node.q_value += self.alpha * (exp_reward + self.gamma * max_child_q - exp_node.q_value)\n",
    "                logging.debug(f\"Updated Q-value for node '{exp_node.description}': {exp_node.q_value}\")\n",
    "\n",
    "    def select_next_node(self):\n",
    "        total_visits = sum(node.visits for _, node in self.queue.queue) + 1  # Avoid division by zero\n",
    "\n",
    "        def ucb_score(node):\n",
    "            if node.visits == 0:\n",
    "                return float('inf')\n",
    "            exploitation = node.q_value\n",
    "            exploration = self.c * math.sqrt(math.log(total_visits) / node.visits)\n",
    "            return exploitation + exploration\n",
    "\n",
    "        if not self.queue.empty():\n",
    "            # Select the node with the highest UCB score\n",
    "            selected_item = max(self.queue.queue, key=lambda x: ucb_score(x[1]))\n",
    "            self.queue.queue.remove(selected_item)\n",
    "            selected_node = selected_item[1]\n",
    "            logging.debug(f\"UCB selected node: {selected_node.description}\")\n",
    "            return selected_node\n",
    "        return None\n",
    "\n",
    "    def fetch_relevant_data(self, node):\n",
    "        # Fetch relevant PDF chunks\n",
    "        node.fetched_pdf = self.executor_module.fetch_pdf_chunks(node.description)\n",
    "        # Fetch relevant data from CSV tables\n",
    "        node.fetched_csv_data = self.executor_module.fetch_csv_data(node.description)\n",
    "        # Log the fetched data for summary\n",
    "        step_detail = {\n",
    "            \"Thought\": node.description,\n",
    "            \"Fetched PDF\": node.fetched_pdf,\n",
    "            \"Fetched CSV Data\": node.fetched_csv_data\n",
    "        }\n",
    "        self.analysis_steps.append(step_detail)\n",
    "\n",
    "    def check_if_answer_possible(self, node):\n",
    "        \"\"\"\n",
    "        Uses the LLM to determine whether the current data is sufficient to answer the student's question.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Checking if answer is possible for node: {node.description}\")\n",
    "\n",
    "        # Limit the number of PDF chunks and CSV data included\n",
    "        max_pdf_chunks = 2  # Adjust as needed\n",
    "        max_csv_entries = 1  # Adjust as needed\n",
    "\n",
    "        # Truncate or summarize the fetched PDF information\n",
    "        fetched_pdf_info = node.fetched_pdf[:max_pdf_chunks] if node.fetched_pdf else []\n",
    "        fetched_pdf_info_text = '\\n'.join(fetched_pdf_info)\n",
    "\n",
    "        # Truncate or summarize the fetched CSV data\n",
    "        fetched_csv_data = [node.fetched_csv_data] if node.fetched_csv_data is not None else []\n",
    "        fetched_csv_data_text = '\\n'.join([str(df.head()) if isinstance(df, pd.DataFrame) else str(df) for df in fetched_csv_data])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert academic advisor.\n",
    "\n",
    "Based on the current thought, the fetched PDF information, and the fetched CSV data, determine whether there is enough information to answer the student's question.\n",
    "\n",
    "Student's question:\n",
    "\"{self.question}\"\n",
    "\n",
    "Current thought:\n",
    "\"{node.description}\"\n",
    "\n",
    "Fetched PDF Information:\n",
    "{fetched_pdf_info_text}\n",
    "\n",
    "Fetched CSV Data:\n",
    "{fetched_csv_data_text}\n",
    "\n",
    "Is there enough information to answer the student's question? Answer \"YES\" or \"NO\" and provide a brief justification.\n",
    "\n",
    "Also, provide a confidence score between 0 and 1 indicating how confident you are that the information is sufficient, where 1 means absolutely certain and 0 means not at all certain.\n",
    "\n",
    "Your response should be in the following format:\n",
    "\n",
    "Answer: [YES/NO]\n",
    "Confidence: [confidence score]\n",
    "Justification: [brief justification]\n",
    "\"\"\"\n",
    "\n",
    "        # Check token length and truncate if necessary\n",
    "        max_total_tokens = 4097\n",
    "        max_completion_tokens = 150\n",
    "        max_prompt_tokens = max_total_tokens - max_completion_tokens\n",
    "\n",
    "        prompt_tokens = self.vector_search.count_tokens(prompt)\n",
    "        if prompt_tokens > max_prompt_tokens:\n",
    "            # Truncate the fetched data further\n",
    "            fetched_pdf_info_text = self.truncate_text(fetched_pdf_info_text, max_length=1000)\n",
    "            fetched_csv_data_text = self.truncate_text(fetched_csv_data_text, max_length=1000)\n",
    "\n",
    "            # Reconstruct the prompt\n",
    "            prompt = f\"\"\"\n",
    "You are an expert academic advisor.\n",
    "\n",
    "Based on the current thought, the fetched PDF information, and the fetched CSV data, determine whether there is enough information to answer the student's question.\n",
    "\n",
    "Student's question:\n",
    "\"{self.question}\"\n",
    "\n",
    "Current thought:\n",
    "\"{node.description}\"\n",
    "\n",
    "Fetched PDF Information:\n",
    "{fetched_pdf_info_text}\n",
    "\n",
    "Fetched CSV Data:\n",
    "{fetched_csv_data_text}\n",
    "\n",
    "Is there enough information to answer the student's question? Answer \"YES\" or \"NO\" and provide a brief justification.\n",
    "\n",
    "Also, provide a confidence score between 0 and 1 indicating how confident you are that the information is sufficient, where 1 means absolutely certain and 0 means not at all certain.\n",
    "\n",
    "Your response should be in the following format:\n",
    "\n",
    "Answer: [YES/NO]\n",
    "Confidence: [confidence score]\n",
    "Justification: [brief justification]\n",
    "\"\"\"\n",
    "            # Recalculate tokens\n",
    "            prompt_tokens = self.vector_search.count_tokens(prompt)\n",
    "            if prompt_tokens > max_prompt_tokens:\n",
    "                logging.error(\"Prompt is still too long even after truncation.\")\n",
    "                node.answer_possible = False\n",
    "                node.answer_confidence = 0\n",
    "                return False\n",
    "\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=150,\n",
    "                temperature=0,\n",
    "            )\n",
    "            answer = response.choices[0].text.strip()\n",
    "            logging.info(f\"LLM response for answer possibility:\\n{answer}\")\n",
    "            match = re.search(r'Answer:\\s*(YES|NO)', answer, re.IGNORECASE)\n",
    "            confidence_match = re.search(r'Confidence:\\s*([0-9]*\\.?[0-9]+)', answer)\n",
    "            if match and confidence_match:\n",
    "                node.answer_possible = True if match.group(1).strip().upper() == 'YES' else False\n",
    "                node.answer_confidence = float(confidence_match.group(1))\n",
    "            else:\n",
    "                node.answer_possible = False\n",
    "                node.answer_confidence = 0\n",
    "            return node.answer_possible\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error checking answer possibility: {e}\")\n",
    "            node.answer_possible = False\n",
    "            node.answer_confidence = 0\n",
    "            return False\n",
    "\n",
    "    def expand_node(self, node):\n",
    "        logging.info(f\"Expanding node: {node.description} at depth {node.depth}\")\n",
    "        node.visits += 1  # Increment visit count\n",
    "        node_id = node.get_unique_id()\n",
    "\n",
    "        # Enhanced Cycle Detection: Check if node has been visited before\n",
    "        if node_id in self.visited_nodes:\n",
    "            logging.info(\"Cycle detected. Skipping node expansion.\")\n",
    "            return\n",
    "        self.visited_nodes.add(node_id)\n",
    "\n",
    "        self.fetch_relevant_data(node)\n",
    "\n",
    "        # Collect answer findings from multiple nodes\n",
    "        if self.check_if_answer_possible(node) and node.answer_confidence >= self.answer_confidence_threshold:\n",
    "            logging.info(\"Answer criteria met with sufficient confidence for this node.\")\n",
    "            node.node_type = 'answer'  # Mark node as an answer node\n",
    "            self.answer_findings.append(node)\n",
    "            self.high_value_paths.add(node.description)  # Add to high-value paths\n",
    "            # Continue exploration to find more information\n",
    "        elif node.depth >= self.min_exploration_depth and self.check_if_answer_possible(node):\n",
    "            # If answer is possible but confidence is low, continue exploration\n",
    "            if node.answer_confidence < self.answer_confidence_threshold:\n",
    "                logging.info(\"Answer possible but confidence is low; continuing exploration.\")\n",
    "\n",
    "        next_thoughts = self.generate_next_thoughts(node)\n",
    "\n",
    "        for thought_description in next_thoughts:\n",
    "            child_node = ThoughtNode(thought_description, parent=node, depth=node.depth + 1)\n",
    "\n",
    "            # Enhanced: Check for cycles before adding child\n",
    "            child_id = child_node.get_unique_id()\n",
    "            if child_id in self.visited_nodes:\n",
    "                logging.info(f\"Cycle detected for child node '{child_node.description}'. Skipping addition.\")\n",
    "                continue\n",
    "\n",
    "            node.add_child(child_node)\n",
    "            child_node.heuristic = self.calculate_heuristic(child_node)\n",
    "            priority = self.compute_priority(child_node)\n",
    "            self.queue.put((priority, child_node))\n",
    "            self.graph.add_node(child_node.description, node=child_node)\n",
    "            self.graph.add_edge(node.description, child_node.description)\n",
    "            logging.info(f\"Added child node: {child_node.description}\")\n",
    "\n",
    "        self.traversed_path.append(node.description)\n",
    "        logging.debug(f\"Current Traversal Path: {self.traversed_path}\")\n",
    "\n",
    "    def generate_next_thoughts(self, node):\n",
    "        prompt_template = f\"\"\"\n",
    "You are an expert academic advisor. Based on the current thought and the information fetched from the PDF and CSV tables, determine the next steps to answer the student's question.\n",
    "\n",
    "Current thought:\n",
    "\"{node.description}\"\n",
    "\n",
    "Fetched PDF Information:\n",
    "{node.fetched_pdf}\n",
    "\n",
    "Fetched CSV Data:\n",
    "{node.fetched_csv_data}\n",
    "\n",
    "Generate {self.breadth} specific next thoughts to proceed with answering the question.\n",
    "\n",
    "Example output:\n",
    "- Thought 1: Review the student's financial aid application status.\n",
    "- Thought 2: Check if the FAFSA was submitted before the priority filing date.\n",
    "- Thought 3: Explore scholarship opportunities available to the student.\n",
    "- Thought 4: Analyze the student's eligibility for work-study programs.\n",
    "\"\"\"\n",
    "\n",
    "        # Limit the size of the fetched data in the prompt\n",
    "        max_pdf_chunks = 2\n",
    "        max_csv_entries = 1\n",
    "\n",
    "        # Adjust fetched PDF and CSV data\n",
    "        fetched_pdf_info = node.fetched_pdf[:max_pdf_chunks] if node.fetched_pdf else []\n",
    "        fetched_pdf_info_text = '\\n'.join(fetched_pdf_info)\n",
    "\n",
    "        fetched_csv_data = [node.fetched_csv_data] if node.fetched_csv_data is not None else []\n",
    "        fetched_csv_data_text = '\\n'.join([str(df.head()) if isinstance(df, pd.DataFrame) else str(df) for df in fetched_csv_data])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert academic advisor. Based on the current thought and the information fetched from the PDF and CSV tables, determine the next steps to answer the student's question.\n",
    "\n",
    "Student's question:\n",
    "\"{self.question}\"\n",
    "\n",
    "Current thought:\n",
    "\"{node.description}\"\n",
    "\n",
    "Fetched PDF Information:\n",
    "{fetched_pdf_info_text}\n",
    "\n",
    "Fetched CSV Data:\n",
    "{fetched_csv_data_text}\n",
    "\n",
    "Generate {self.breadth} specific next thoughts to proceed with answering the question.\n",
    "\n",
    "Example output:\n",
    "- Thought 1: Review the student's financial aid application status.\n",
    "- Thought 2: Check if the FAFSA was submitted before the priority filing date.\n",
    "- Thought 3: Explore scholarship opportunities available to the student.\n",
    "- Thought 4: Analyze the student's eligibility for work-study programs.\n",
    "\"\"\"\n",
    "\n",
    "        max_total_tokens = 4097  # Model's maximum context length\n",
    "        max_completion_tokens = 500\n",
    "        max_prompt_tokens = max_total_tokens - max_completion_tokens\n",
    "\n",
    "        prompt_tokens = self.vector_search.count_tokens(prompt)\n",
    "        if prompt_tokens > max_prompt_tokens:\n",
    "            # Truncate the fetched data\n",
    "            fetched_pdf_info_text = self.truncate_text(fetched_pdf_info_text, max_length=1000)\n",
    "            fetched_csv_data_text = self.truncate_text(fetched_csv_data_text, max_length=1000)\n",
    "            # Reconstruct the prompt\n",
    "            prompt = f\"\"\"\n",
    "You are an expert academic advisor. Based on the current thought and the information fetched from the PDF and CSV tables, determine the next steps to answer the student's question.\n",
    "\n",
    "Student's question:\n",
    "\"{self.question}\"\n",
    "\n",
    "Current thought:\n",
    "\"{node.description}\"\n",
    "\n",
    "Fetched PDF Information:\n",
    "{fetched_pdf_info_text}\n",
    "\n",
    "Fetched CSV Data:\n",
    "{fetched_csv_data_text}\n",
    "\n",
    "Generate {self.breadth} specific next thoughts to proceed with answering the question.\n",
    "\n",
    "Example output:\n",
    "- Thought 1: Review the student's financial aid application status.\n",
    "- Thought 2: Check if the FAFSA was submitted before the priority filing date.\n",
    "- Thought 3: Explore scholarship opportunities available to the student.\n",
    "- Thought 4: Analyze the student's eligibility for work-study programs.\n",
    "\"\"\"\n",
    "            prompt_tokens = self.vector_search.count_tokens(prompt)\n",
    "            if prompt_tokens > max_prompt_tokens:\n",
    "                logging.error(\"Prompt is too long even after truncation.\")\n",
    "                return [\"Review current information for additional insights\"]\n",
    "\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=500,\n",
    "                temperature=0,\n",
    "            )\n",
    "            next_thoughts_text = response.choices[0].text.strip()\n",
    "            logging.debug(f\"Received next thoughts from API: {next_thoughts_text}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in API call: {e}\")\n",
    "            next_thoughts_text = \"Unable to generate next thoughts due to an error.\"\n",
    "\n",
    "        next_thoughts = []\n",
    "        for line in next_thoughts_text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('- Thought'):\n",
    "                thought_description = line.split(':', 1)[1].strip() if ':' in line else line.replace('- Thought', '').strip()\n",
    "                if thought_description and thought_description not in next_thoughts:\n",
    "                    next_thoughts.append(thought_description)\n",
    "\n",
    "        if not next_thoughts:\n",
    "            next_thoughts.append(\"Review current information for additional insights\")\n",
    "\n",
    "        # Limit to the configured number of thoughts\n",
    "        return next_thoughts[:self.breadth]\n",
    "\n",
    "    def calculate_reward(self, node):\n",
    "        # Reward for fetched data\n",
    "        data_reward = len(node.fetched_pdf) if node.fetched_pdf else 0\n",
    "        csv_reward = 1 if node.fetched_csv_data is not None else 0\n",
    "\n",
    "        # Additional reward if the node meets answer criteria with high confidence\n",
    "        if node.answer_possible and node.answer_confidence >= self.answer_confidence_threshold:\n",
    "            answer_reward = 10.0 * node.answer_confidence\n",
    "        else:\n",
    "            answer_reward = 0.0\n",
    "\n",
    "        # Penalty for shallow depth conclusions\n",
    "        depth_penalty = -1.0 if node.depth < self.min_exploration_depth else 0.0\n",
    "\n",
    "        # Final reward calculation\n",
    "        total_reward = data_reward + csv_reward + answer_reward + depth_penalty\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    @staticmethod\n",
    "    def truncate_text(text, max_length):\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        else:\n",
    "            return text[:max_length//2] + \"\\n[...]\\n\" + text[-max_length//2:]\n",
    "\n",
    "    def adjust_epsilon(self, iterations_without_improvement):\n",
    "        # Adaptive epsilon adjustment based on iterations without improvement\n",
    "        if iterations_without_improvement > 0 and iterations_without_improvement % 10 == 0:\n",
    "            # Every 10 iterations without improvement, increase epsilon\n",
    "            old_epsilon = self.epsilon\n",
    "            self.epsilon = min(1.0, self.epsilon + 0.05)\n",
    "            logging.debug(f\"Increased epsilon from {old_epsilon} to {self.epsilon} due to lack of improvement.\")\n",
    "        else:\n",
    "            # Otherwise, decay epsilon\n",
    "            old_epsilon = self.epsilon\n",
    "            self.epsilon = max(0.1, self.epsilon * self.epsilon_decay_factor)\n",
    "            logging.debug(f\"Decayed epsilon from {old_epsilon} to {self.epsilon}\")\n",
    "\n",
    "    def generate_answer(self, node):\n",
    "        logging.info(f\"Generating answer for node: {node.description}\")\n",
    "\n",
    "        # Mark nodes in the traversed path\n",
    "        current = node\n",
    "        while current:\n",
    "            current.node_type = 'path'\n",
    "            current = current.parent\n",
    "\n",
    "        # Collect all available data with limits\n",
    "        available_data = self.collect_analysis(node, max_thoughts=5)\n",
    "\n",
    "        # Process PDF data\n",
    "        fetched_pdf_info_text = '\\n'.join(available_data['Fetched PDF'] if available_data['Fetched PDF'] else [])\n",
    "        fetched_pdf_info_text = self.truncate_text(fetched_pdf_info_text, max_length=1000)\n",
    "\n",
    "        # Process CSV data more carefully\n",
    "        fetched_csv_data_text = \"\"\n",
    "        try:\n",
    "            csv_data = available_data['Fetched CSV Data']\n",
    "            if csv_data:\n",
    "                processed_dfs = []\n",
    "                for df in csv_data:\n",
    "                    if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                        # Make a copy and clean up column names\n",
    "                        temp_df = df.copy()\n",
    "                        # Filter for student ID\n",
    "                        temp_df = temp_df[temp_df['Student_ID'] == self.student_id]\n",
    "                        if not temp_df.empty:\n",
    "                            # Remove duplicate columns\n",
    "                            temp_df = temp_df.loc[:, ~temp_df.columns.duplicated()]\n",
    "                            processed_dfs.append(temp_df)\n",
    "\n",
    "                if processed_dfs:\n",
    "                    # Combine all DataFrames\n",
    "                    try:\n",
    "                        combined_df = pd.concat(processed_dfs, axis=1)\n",
    "                        # Remove duplicate columns again after concatenation\n",
    "                        combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
    "                        fetched_csv_data_text = combined_df.to_string()\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error concatenating DataFrames: {e}\")\n",
    "                        # Fallback: just convert each DataFrame to string separately\n",
    "                        fetched_csv_data_text = \"\\n\".join(df.to_string() for df in processed_dfs)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing CSV data: {e}\")\n",
    "            fetched_csv_data_text = \"Error processing CSV data\"\n",
    "\n",
    "        # Truncate CSV text if needed\n",
    "        fetched_csv_data_text = self.truncate_text(fetched_csv_data_text, max_length=1000)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "    You are an expert academic advisor. Based on the analysis below, provide a detailed answer to the student's question, incorporating specific data related to Student ID {self.student_id}.\n",
    "\n",
    "    Student's question:\n",
    "    \"{self.question}\"\n",
    "\n",
    "    Thought Process:\n",
    "    {available_data['Thoughts']}\n",
    "\n",
    "    Fetched PDF Information:\n",
    "    {fetched_pdf_info_text}\n",
    "\n",
    "    Fetched CSV Data for Student ID {self.student_id}:\n",
    "    {fetched_csv_data_text}\n",
    "\n",
    "    Provide a clear and detailed answer to the student's question, using the information above, and reference the specific data for Student ID {self.student_id}.\n",
    "    \"\"\"\n",
    "\n",
    "        # Check token length and adjust if necessary\n",
    "        max_total_tokens = 4097\n",
    "        max_completion_tokens = 500\n",
    "        max_prompt_tokens = max_total_tokens - max_completion_tokens\n",
    "\n",
    "        prompt_tokens = self.vector_search.count_tokens(prompt)\n",
    "        if prompt_tokens > max_prompt_tokens:\n",
    "            # Truncate further if needed\n",
    "            fetched_pdf_info_text = self.truncate_text(fetched_pdf_info_text, max_length=500)\n",
    "            fetched_csv_data_text = self.truncate_text(fetched_csv_data_text, max_length=500)\n",
    "            # Reconstruct prompt with truncated data\n",
    "            prompt = f\"\"\"\n",
    "    You are an expert academic advisor. Based on the analysis below, provide a detailed answer to the student's question, incorporating specific data related to Student ID {self.student_id}.\n",
    "\n",
    "    Student's question:\n",
    "    \"{self.question}\"\n",
    "\n",
    "    Thought Process:\n",
    "    {available_data['Thoughts']}\n",
    "\n",
    "    Fetched PDF Information:\n",
    "    {fetched_pdf_info_text}\n",
    "\n",
    "    Fetched CSV Data for Student ID {self.student_id}:\n",
    "    {fetched_csv_data_text}\n",
    "\n",
    "    Provide a clear and detailed answer to the student's question, using the information above, and reference the specific data for Student ID {self.student_id}.\n",
    "    \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=500,\n",
    "                temperature=0,\n",
    "            )\n",
    "            answer = response.choices[0].text.strip()\n",
    "            logging.info(f\"Answer generated: {answer}\")\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating answer: {e}\")\n",
    "            return \"I apologize, but I encountered an error while generating the answer. Please try asking your question again.\"\n",
    "\n",
    "    def collect_analysis(self, node, max_thoughts=5):\n",
    "        thoughts = []\n",
    "        fetched_pdf = []\n",
    "        fetched_csv_data = []\n",
    "        current = node\n",
    "        count = 0\n",
    "        while current and count < max_thoughts:\n",
    "            thoughts.append(f\"- {current.description}\")\n",
    "            if current.fetched_pdf:\n",
    "                fetched_pdf.extend(current.fetched_pdf)\n",
    "            if current.fetched_csv_data is not None:\n",
    "                fetched_csv_data.append(current.fetched_csv_data)\n",
    "            current = current.parent\n",
    "            count += 1\n",
    "        analysis = {\n",
    "            \"Thoughts\": '\\n'.join(reversed(thoughts)),\n",
    "            \"Fetched PDF\": fetched_pdf,\n",
    "            \"Fetched CSV Data\": fetched_csv_data  # Ensure this is a list of DataFrames\n",
    "        }\n",
    "        return analysis\n",
    "\n",
    "    def print_traversed_path(self, node):\n",
    "        # Collect path from root to the given node\n",
    "        path = []\n",
    "        current = node\n",
    "        while current:\n",
    "            path.append(current.description)\n",
    "            current = current.parent\n",
    "        path = list(reversed(path))\n",
    "        print(\"\\nTraversed Path:\")\n",
    "        for step in path:\n",
    "            print(f\"- {step}\")\n",
    "\n",
    "    def run_analysis(self):\n",
    "        logging.info(\"Starting analysis\")\n",
    "        self.root.heuristic = self.calculate_heuristic(self.root)\n",
    "        priority = self.compute_priority(self.root)\n",
    "        self.queue.put((priority, self.root))\n",
    "\n",
    "        best_q_value = float('-inf')\n",
    "        self.iterations_without_improvement = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        min_time = 10  # Adjusted minimum time\n",
    "        max_time = 20  # Maximum time as per requirement\n",
    "\n",
    "        while not self.queue.empty() and self.iterations < self.max_iterations:\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            if elapsed_time > max_time:\n",
    "                logging.info(\"Maximum time limit reached; terminating analysis.\")\n",
    "                break\n",
    "\n",
    "            node = self.select_next_node()\n",
    "            if node is None:\n",
    "                logging.warning(\"No node selected; terminating analysis.\")\n",
    "                break\n",
    "\n",
    "            self.expand_node(node)\n",
    "            self.iterations += 1\n",
    "            logging.info(f\"Iteration {self.iterations}: Expanded node '{node.description}' with Q-value {node.q_value}\")\n",
    "\n",
    "            reward = self.calculate_reward(node)\n",
    "            self.update_q_value(node, reward)\n",
    "            logging.info(f\"Updated Q-value for node '{node.description}': {node.q_value}\")\n",
    "\n",
    "            # Track best Q-value for convergence\n",
    "            if node.q_value > best_q_value:\n",
    "                best_q_value = node.q_value\n",
    "                self.iterations_without_improvement = 0\n",
    "                logging.info(f\"New best Q-value found: {best_q_value}\")\n",
    "            else:\n",
    "                self.iterations_without_improvement += 1\n",
    "                logging.info(f\"No improvement in Q-value. Iterations without improvement: {self.iterations_without_improvement}\")\n",
    "\n",
    "            # Adjust parameters adaptively\n",
    "            self.adjust_epsilon(self.iterations_without_improvement)\n",
    "\n",
    "            # Ensure minimum run time\n",
    "            if elapsed_time < min_time:\n",
    "                continue\n",
    "\n",
    "            # Early convergence conditions\n",
    "            if len(self.answer_findings) >= 1 and elapsed_time >= min_time:\n",
    "                logging.info(\"Answer found; terminating analysis early.\")\n",
    "                break\n",
    "\n",
    "            if self.iterations_without_improvement > self.iteration_limit_without_improvement and elapsed_time >= min_time:\n",
    "                logging.info(f\"No improvement over {self.iteration_limit_without_improvement} iterations; terminating analysis.\")\n",
    "                break\n",
    "\n",
    "            # Log performance metrics\n",
    "            logging.info(f\"Iteration {self.iterations}: Elapsed Time: {elapsed_time:.2f}s, Epsilon: {self.epsilon:.4f}, Alpha: {self.alpha:.4f}, Gamma: {self.gamma:.4f}\")\n",
    "\n",
    "        # After search, generate the answer\n",
    "        if self.answer_findings:\n",
    "            answer_node = self.answer_findings[-1]\n",
    "            answer = self.generate_answer(answer_node)\n",
    "            if answer:\n",
    "                print(\"Answer to your question:\")\n",
    "                print(answer)\n",
    "            else:\n",
    "                print(\"Unable to provide a definitive answer with the available information.\")\n",
    "\n",
    "            # Print the traversed path\n",
    "            self.print_traversed_path(answer_node)\n",
    "        else:\n",
    "            print(\"Unable to determine a definitive answer with the available information.\")\n",
    "\n",
    "        # Visualize the graph after analysis\n",
    "        self.visualize_graph()\n",
    "\n",
    "    def visualize_graph(self):\n",
    "        # Position nodes using spring layout\n",
    "        pos = nx.spring_layout(self.graph, seed=42)\n",
    "\n",
    "        # Prepare node attributes\n",
    "        node_colors = []\n",
    "        node_shapes = {}\n",
    "        for node_name in self.graph.nodes:\n",
    "            node = self.graph.nodes[node_name]['node']\n",
    "            if node.node_type == 'root':\n",
    "                node_colors.append('lightgreen')\n",
    "                node_shapes[node_name] = 'o'  # Circle\n",
    "            elif node.node_type == 'answer':\n",
    "                node_colors.append('gold')\n",
    "                node_shapes[node_name] = 's'  # Square\n",
    "            elif node.node_type == 'path':\n",
    "                node_colors.append('skyblue')\n",
    "                node_shapes[node_name] = 'D'  # Diamond\n",
    "            else:\n",
    "                node_colors.append('lightgray')\n",
    "                node_shapes[node_name] = 'o'  # Circle\n",
    "\n",
    "        # Draw nodes with different shapes\n",
    "        unique_shapes = set(node_shapes.values())\n",
    "        for shape in unique_shapes:\n",
    "            nodes_with_shape = [node_name for node_name in self.graph.nodes if node_shapes[node_name] == shape]\n",
    "            nx.draw_networkx_nodes(\n",
    "                self.graph,\n",
    "                pos,\n",
    "                nodelist=nodes_with_shape,\n",
    "                node_color=[node_colors[list(self.graph.nodes).index(node)] for node in nodes_with_shape],\n",
    "                node_shape=shape,\n",
    "                node_size=3000\n",
    "            )\n",
    "\n",
    "        # Draw edges and labels\n",
    "        nx.draw_networkx_edges(self.graph, pos, arrows=True, arrowstyle='-|>', arrowsize=12)\n",
    "        labels = {node_name: f\"{node_name}\\nQ: {self.graph.nodes[node_name]['node'].q_value:.2f}\" for node_name in self.graph.nodes}\n",
    "        nx.draw_networkx_labels(self.graph, pos, labels=labels, font_size=8, font_weight='bold')\n",
    "\n",
    "        plt.title('Graph of Thoughts Visualization')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('graph_of_thoughts.png')  # Save the figure\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    question = input(\"Please enter your question: \")\n",
    "    student_id_input = input(\"Please enter your Student ID: \")\n",
    "    try:\n",
    "        student_id = int(student_id_input)\n",
    "    except ValueError:\n",
    "        print(\"Invalid Student ID. Please enter a numeric Student ID.\")\n",
    "        return\n",
    "\n",
    "    data_loader = DataLoader()\n",
    "    vector_search = VectorSearch()\n",
    "    vector_search.load_or_build_pdf_vectors(data_loader.pdf_texts)\n",
    "\n",
    "    config = {\n",
    "        'iteration_limit': 1000,\n",
    "        'iteration_limit_without_improvement': 100,\n",
    "        'max_iterations': 800,\n",
    "        'max_depth': 25,\n",
    "        'recurring_threshold': 3,\n",
    "        'min_exploration_depth': 3,\n",
    "        'answer_confidence_threshold': 0.7,\n",
    "        'convergence_threshold': 0.005,\n",
    "        'breadth': 7,\n",
    "        'visualization_interval': 15,\n",
    "        'replay_buffer_size': 100,\n",
    "        'exploration_constant': 1.9,  # UCB exploration parameter\n",
    "        'target_q_update_frequency': 100  # Frequency to update target network\n",
    "    }\n",
    "\n",
    "    got_manager = GraphOfThoughts(question, data_loader, vector_search, student_id, config=config)\n",
    "    got_manager.run_analysis()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juspay39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
